# Prefect Medallion Architecture Implementation

## Overview
This directory contains the **Medallion Architecture** implementation using Prefect for workflow orchestration. It follows the **Bronze â†’ Silver â†’ Gold** pattern for data processing with local development capabilities.

## Architecture Components

### ğŸ¥‰ Bronze Layer (Raw Data)
- **Data Sources**: Polygon.io API, Yahoo Finance (fallback)
- **Storage**: PostgreSQL/TimescaleDB for raw OHLCV data
- **Processing**: Data extraction and initial validation
- **Schedule**: Daily after market close

### ğŸ¥ˆ Silver Layer (Processed Data)
- **Processing**: Data cleaning, resampling, and aggregation
- **Storage**: DuckDB for analytical processing
- **Features**: Multiple timeframe resampling (1m, 3m, 5m, 8m, 13m, 21m, 34m, 55m)
- **Output**: Clean OHLCV data ready for analysis

### ğŸ¥‡ Gold Layer (Analytics)
- **Processing**: Technical indicators and trading signals
- **Strategies**: Vegas Channel, custom indicators
- **Storage**: Parquet files for fast analytics
- **Output**: Trading signals and performance metrics

## Directory Structure

```
prefect_medallion/
â”œâ”€â”€ flows/                  # Prefect flow definitions
â”‚   â”œâ”€â”€ make_bronze_pipeline.py    # Bronze layer flow
â”‚   â”œâ”€â”€ make_silver_pipeline.py    # Silver layer flow
â”‚   â””â”€â”€ make_gold_pipeline.py      # Gold layer flow
â”œâ”€â”€ fetch/                  # Data extraction modules
â”‚   â”œâ”€â”€ batch.py           # Batch data extraction
â”‚   â”œâ”€â”€ meta.py            # Symbol metadata extraction
â”‚   â””â”€â”€ stream.py          # Real-time data streaming
â”œâ”€â”€ ingest/                 # Data ingestion modules
â”‚   â”œâ”€â”€ batch.py           # Batch data ingestion
â”‚   â””â”€â”€ meta.py            # Metadata ingestion
â”œâ”€â”€ process/                # Data processing modules
â”‚   â”œâ”€â”€ core/              # Core processing logic
â”‚   â”‚   â”œâ”€â”€ data_loader.py # Data loading utilities
â”‚   â”‚   â”œâ”€â”€ resampler.py   # Time series resampling
â”‚   â”‚   â”œâ”€â”€ indicator.py   # Technical indicators
â”‚   â”‚   â”œâ”€â”€ cache.py       # Caching mechanisms
â”‚   â”‚   â””â”€â”€ alert_rules.py # Alert and notification rules
â”‚   â”œâ”€â”€ strategies/        # Trading strategies
â”‚   â”‚   â””â”€â”€ VegasChannel.py # Vegas Channel strategy
â”‚   â”œâ”€â”€ sql/               # SQL queries and views
â”‚   â”‚   â”œâ”€â”€ resample_view.sql      # Resampling views
â”‚   â”‚   â””â”€â”€ serializability_check.sql # Data validation
â”‚   â””â”€â”€ storage/           # Data storage
â”‚       â”œâ”€â”€ silver/        # Silver layer storage (DuckDB)
â”‚       â””â”€â”€ gold/          # Gold layer storage (Parquet)
â”œâ”€â”€ tools/                  # Utility modules
â”‚   â”œâ”€â”€ polygon_client.py  # Polygon.io API client
â”‚   â”œâ”€â”€ postgres_client.py # PostgreSQL client
â”‚   â”œâ”€â”€ redis_client.py    # Redis client
â”‚   â”œâ”€â”€ kafka_client.py    # Kafka client
â”‚   â””â”€â”€ utils.py           # General utilities
â”œâ”€â”€ config/                 # Configuration management
â”‚   â”œâ”€â”€ settings.yaml      # Application settings
â”‚   â””â”€â”€ load_setting.py    # Settings loader
â”œâ”€â”€ database/               # Database schemas and scripts
â”‚   â”œâ”€â”€ schema.sql         # Database schema definitions
â”‚   â”œâ”€â”€ example.sql        # Example queries
â”‚   â”œâ”€â”€ deletion.sql       # Data cleanup scripts
â”‚   â””â”€â”€ sql_analysis.sql   # Analysis queries
â”œâ”€â”€ jupyter_notebook/       # Analysis notebooks
â”‚   â”œâ”€â”€ tools_functionality.ipynb # Tools testing
â”‚   â”œâ”€â”€ silver_loading.ipynb      # Silver layer analysis
â”‚   â”œâ”€â”€ gold_loading.ipynb        # Gold layer analysis
â”‚   â”œâ”€â”€ olap.ipynb               # OLAP queries
â”‚   â””â”€â”€ rest_api.ipynb           # API testing
â”œâ”€â”€ commands/               # Utility commands
â”‚   â”œâ”€â”€ prefect.sh         # Prefect management commands
â”‚   â”œâ”€â”€ kafka.cmd          # Kafka management
â”‚   â”œâ”€â”€ commands.sql       # Database commands
â”‚   â””â”€â”€ mongdb.java        # MongoDB utilities
â””â”€â”€ test/                   # Test modules
    â””â”€â”€ debug_save_gold.py  # Gold layer testing
```

## Key Features

### ğŸ”„ Data Processing Pipeline
1. **Bronze Pipeline**: Raw data extraction from multiple sources
2. **Silver Pipeline**: Data cleaning and resampling
3. **Gold Pipeline**: Analytics and signal generation

### âš¡ Real-time Capabilities
- Kafka-based streaming architecture
- Redis caching for fast access
- Real-time signal generation

### ğŸ“Š Analytics & Strategies
- **Vegas Channel Strategy**: Trend-following strategy
- **Custom Indicators**: RSI, MACD, Bollinger Bands
- **Multi-timeframe Analysis**: 1m to 55m intervals

### ğŸ› ï¸ Development Tools
- **Jupyter Notebooks**: Interactive analysis and development
- **Docker Support**: Containerized deployment
- **Configuration Management**: YAML-based settings

## Getting Started

### Prerequisites
- Python 3.8+
- PostgreSQL/TimescaleDB
- Redis (optional)
- Kafka (optional, for streaming)

### Installation
```bash
cd prefect_medallion/

# Create virtual environment
python -m venv .dp
source .dp/bin/activate  # On Windows: .dp\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Setup environment variables
cp ../.env.example ../.env
# Edit .env with your credentials
```

### Local Development Setup
```bash
# Start Prefect server
prefect server start --host 127.0.0.1 --port 4200

# In another terminal, run flows
source .dp/bin/activate
python flows/make_bronze_pipeline.py
```

### Configuration
Edit `config/settings.yaml` to customize:
- Data extraction parameters
- Processing intervals
- Storage configurations
- Strategy parameters

## Usage Examples

### Running Individual Flows
```bash
# Bronze layer (data extraction)
python flows/make_bronze_pipeline.py

# Silver layer (data processing)
python flows/make_silver_pipeline.py

# Gold layer (analytics)
python flows/make_gold_pipeline.py
```

### Interactive Analysis
```bash
# Start Jupyter
jupyter notebook

# Open analysis notebooks
# - tools_functionality.ipynb: Test tools and connections
# - silver_loading.ipynb: Analyze processed data
# - gold_loading.ipynb: Review trading signals
```

### Strategy Development
```bash
# Add new strategy to process/strategies/
# Update flows/make_gold_pipeline.py
# Test in jupyter_notebook/gold_loading.ipynb
```

## Monitoring & Debugging

### Prefect UI
- Access: http://localhost:4200
- Monitor flow runs and task status
- View logs and metrics

### Data Validation
```sql
-- Check data completeness
SELECT * FROM process/sql/serializability_check.sql

-- Analyze resampled data
SELECT * FROM process/sql/resample_view.sql
```

### Performance Monitoring
- Check DuckDB query performance
- Monitor PostgreSQL connections
- Redis cache hit rates

## Deployment

### Docker Deployment
```bash
# Build container
docker build -t condvest-medallion .

# Run with docker-compose
docker-compose up -d
```

### Production Considerations
- Database connection pooling
- Redis cluster for high availability
- Kafka cluster for streaming
- Monitoring and alerting setup

## Troubleshooting

### Common Issues
1. **Prefect Server Connection**: Ensure server is running on port 4200
2. **Database Connection**: Check PostgreSQL credentials in .env
3. **API Rate Limits**: Polygon.io has rate limits, use delays
4. **Memory Usage**: DuckDB processing can be memory-intensive

### Debug Tools
- Use `test/debug_save_gold.py` for testing gold layer
- Check `jupyter_notebook/tools_functionality.ipynb` for API testing
- Review Prefect logs in the UI

## Performance Optimization

### Data Processing
- Batch processing for efficiency
- Parallel processing where possible
- Optimized SQL queries

### Storage
- TimescaleDB for time-series data
- DuckDB for analytical workloads
- Parquet for compressed analytics storage

## Contributing

1. Follow the medallion architecture pattern
2. Add tests for new features
3. Update configuration documentation
4. Test with realistic data volumes

## Architecture Benefits

### Pros
- **Familiar Pattern**: Traditional ETL approach
- **Local Development**: Full local testing capability
- **Flexible**: Easy to modify and extend
- **Cost Effective**: No cloud costs for development

### Cons
- **Manual Scaling**: Requires infrastructure management
- **Higher Latency**: Batch-oriented processing
- **Maintenance**: More operational overhead than serverless

## Integration with AWS Lambda Architecture

This implementation serves as:
- **Development Environment**: Test strategies locally
- **Backup Processing**: Alternative to cloud processing
- **Analysis Platform**: Deep dive analytics and research
- **Strategy Development**: Prototype new trading strategies

For production real-time processing, consider the AWS Lambda Architecture implementation in `../aws_lambda_architecture/`.
