{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import yfinance as yf\n",
    "import polygon \n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the timescaledb database\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get the postgres url\n",
    "postgres_url = os.environ.get('POSTGRES_URL')\n",
    "user = os.environ.get('POSTGRES_USER')\n",
    "password = os.environ.get('POSTGRES_PASSWORD')\n",
    "intervals = [1, 3, 5, 8, 13]\n",
    "# Connect to the postgres database\n",
    "try:\n",
    "    conn = psycopg2.connect(postgres_url)\n",
    "    cursor = conn.cursor()\n",
    "    print(\"Connected to the timescaledb database\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to the database: {e}\")\n",
    "    conn = None\n",
    "    cursor = None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.9 s, sys: 4.77 s, total: 19.6 s\n",
      "Wall time: 22.5 s\n"
     ]
    }
   ],
   "source": [
    "# Connect to TimescaleDB\n",
    "conn = psycopg2.connect(postgres_url)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query TimescaleDB\n",
    "query = \"SELECT * FROM raw ORDER BY symbol, date DESC\"\n",
    "cursor.execute(query)\n",
    "\n",
    "# Fetch results\n",
    "%time results = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 s, sys: 9.59 s, total: 20.7 s\n",
      "Wall time: 20 s\n"
     ]
    }
   ],
   "source": [
    "## Connect Polar to Timescale Postgres\n",
    "duck_query = duckdb.sql(f\"\"\"\n",
    "    -- INSTALL postgres_scanner;\n",
    "    -- LOAD postgres_scanner;\n",
    "\n",
    "    SELECT * FROM postgres_scan(\n",
    "        'host=localhost port=5432 user={user} password={password} dbname=condvest',\n",
    "        'public', 'raw'\n",
    "    ) ORDER BY symbol, date DESC;\n",
    "\"\"\")\n",
    "\n",
    "%time duck_df = duck_query.df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.13 ms, sys: 1.25 ms, total: 2.38 ms\n",
      "Wall time: 30 ms\n"
     ]
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "WITH raw_data AS (\n",
    "    SELECT * FROM postgres_scan(\n",
    "        'host=localhost port=5432 user={user} password={password} dbname=condvest',\n",
    "        'public', 'raw' \n",
    "    )\n",
    "),\n",
    "ranked AS (\n",
    "    SELECT *,\n",
    "        row_number() OVER (PARTITION BY symbol ORDER BY date) as rn\n",
    "    FROM raw_data\n",
    "),\n",
    "grouped AS (\n",
    "    SELECT *,\n",
    "        (rn - 1) / 3 as group_id\n",
    "    FROM ranked\n",
    ")\n",
    "SELECT \n",
    "    symbol,\n",
    "    min(date) as date,\n",
    "    first(open) as open,\n",
    "    max(high) as high,\n",
    "    min(low) as low,\n",
    "    last(close) as close,\n",
    "    sum(volume) as volume\n",
    "FROM grouped\n",
    "GROUP BY symbol, group_id\n",
    "ORDER BY symbol, date;\n",
    "\"\"\"\n",
    "\n",
    "%time duckdb_result = duckdb.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Load from Timescale using ConnectorX\n",
    "polars_df = pl.read_database_uri(\n",
    "    \"SELECT date, symbol, open, high, low, close, volume FROM raw ORDER BY symbol ASC, date ASC\",\n",
    "    uri=postgres_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure datetime column is properly cast\n",
    "polars_df = polars_df.with_columns([\n",
    "    pl.col(\"date\").cast(pl.Datetime(\"us\"))\n",
    "])\n",
    "\n",
    "# Resample to 3-day OHLCV candles using Polars' groupby_dynamic\n",
    "resampled_3d_df = (\n",
    "    polars_df.group_by_dynamic(\n",
    "        index_column=\"date\",\n",
    "        every=\"3d\",\n",
    "        by=\"symbol\",\n",
    "        closed=\"left\",\n",
    "        period=\"3d\"\n",
    "    )\n",
    "    .agg([\n",
    "        pl.col(\"open\").first().alias(\"open\"),\n",
    "        pl.col(\"high\").max().alias(\"high\"),\n",
    "        pl.col(\"low\").min().alias(\"low\"),\n",
    "        pl.col(\"close\").last().alias(\"close\"),\n",
    "        pl.col(\"volume\").sum().alias(\"volume\")\n",
    "    ])\n",
    "    .sort([\"symbol\", \"date\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_3d_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. DuckDB + Polar Add Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import polars as pl\n",
    "import time\n",
    "\n",
    "# Step 1: Connect and load Postgres data into DuckDB\n",
    "con = duckdb.connect()\n",
    "\n",
    "combined_results = []\n",
    "intervals = [1,3,5,8,13]\n",
    "for interval in intervals:\n",
    "    start_time = time.time()\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH raw_data AS (\n",
    "        SELECT * FROM postgres_scan(\n",
    "            'host=localhost port=5432 user={user} password={password} dbname=condvest',\n",
    "            'public', 'raw'\n",
    "        )\n",
    "    ),\n",
    "    ranked AS (\n",
    "        SELECT *,\n",
    "            row_number() OVER (PARTITION BY symbol ORDER BY date) as rn\n",
    "        FROM raw_data\n",
    "    ),\n",
    "    grouped AS (\n",
    "        SELECT *,\n",
    "            (rn - 1) / {interval} as group_id\n",
    "        FROM ranked\n",
    "    )\n",
    "    SELECT \n",
    "        symbol,\n",
    "        min(date) as date,\n",
    "        first(open) as open,\n",
    "        max(high) as high,\n",
    "        min(low) as low,\n",
    "        last(close) as close,\n",
    "        sum(volume) as volume,\n",
    "        '{interval}'::INT as interval\n",
    "    FROM grouped\n",
    "    GROUP BY symbol, group_id\n",
    "    ORDER BY symbol, date;\n",
    "    \"\"\"\n",
    "    \n",
    "    df = duckdb.sql(query)\n",
    "    print(f\"DuckDB query execution time: {time.time() - start_time:.2f} seconds\")\n",
    "    combined_results.append(df.df())\n",
    "    print(f\"Duck to dataframe conversion time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Step 2: Convert to Polars DataFrame\n",
    "start_time = time.time()\n",
    "pl_resampled_df = pl.from_pandas(pd.concat(combined_results))\n",
    "print(f\"Pandas to Polars conversion time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Step 3: Convert to Polars and add indicators\n",
    "def add_indicators(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    df = df.sort(\"date\")\n",
    "    \n",
    "    # Step 1: Compute EMAs\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"close\").ewm_mean(span=8).alias(\"EMA_8\"),\n",
    "        pl.col(\"close\").ewm_mean(span=13).alias(\"EMA_13\"),\n",
    "        pl.col(\"close\").ewm_mean(span=21).alias(\"EMA_21\"),\n",
    "        pl.col(\"close\").ewm_mean(span=144).alias(\"EMA_144\"),\n",
    "        pl.col(\"close\").ewm_mean(span=169).alias(\"EMA_169\"),\n",
    "        pl.col(\"close\").ewm_mean(span=55).alias(\"EMA_55\"),\n",
    "        pl.col(\"close\").ewm_mean(span=89).alias(\"EMA_89\"),\n",
    "    ])\n",
    "    \n",
    "    # Step 2: Compute MACD and RSI using already-created columns\n",
    "    df = df.with_columns([\n",
    "        (pl.col(\"EMA_13\") - pl.col(\"EMA_21\")).alias(\"macd_fast\"),\n",
    "        (pl.col(\"EMA_55\") - pl.col(\"EMA_89\")).alias(\"macd_slow\"),\n",
    "    ])\n",
    "    \n",
    "    return df\n",
    "\n",
    "start_time = time.time()\n",
    "df_with_indicators = pl_resampled_df.group_by(\"symbol\", maintain_order=True).map_groups(add_indicators)\n",
    "print(f\"Indicator calculation time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nFirst 10 rows of result:\")\n",
    "print(df_with_indicators.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add Alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "# Define intervals and rolling window\n",
    "intervals = [1, 3, 5, 8, 13]\n",
    "rolling_window = 50\n",
    "\n",
    "# Step 1: Add velocity alerts\n",
    "def add_velocity_alert(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Add velocity alerts based on the relationship between price and various EMAs.\n",
    "    \"\"\"\n",
    "    df = df.with_columns([\n",
    "        pl.when(\n",
    "            (pl.col(\"close\") > pl.col(\"open\")) & \n",
    "            (pl.col(\"close\") > pl.max_horizontal(\"EMA_8\", \"EMA_13\")) & \n",
    "            (pl.col(\"close\") > pl.max_horizontal(\"EMA_144\", \"EMA_169\")) &\n",
    "            (pl.min_horizontal(\"EMA_8\", \"EMA_13\") > pl.max_horizontal(\"EMA_144\", \"EMA_169\"))\n",
    "        ).then(pl.lit(\"velocity_maintained\"))\n",
    "        .when(\n",
    "            (pl.col(\"close\") < pl.col(\"EMA_13\")) & \n",
    "            (pl.col(\"close\") > pl.col(\"EMA_169\"))\n",
    "        ).then(pl.lit(\"velocity_weak\"))\n",
    "        .when(\n",
    "            (pl.col(\"close\") < pl.col(\"EMA_13\")) & \n",
    "            (pl.col(\"close\") < pl.col(\"EMA_169\"))\n",
    "        ).then(pl.lit(\"velocity_loss\"))\n",
    "        .otherwise(pl.lit(\"velocity_negotiating\"))\n",
    "        .alias(\"velocity_status\")\n",
    "    ])\n",
    "    return df\n",
    "\n",
    "# Step 2: Add acceleration/deceleration alerts\n",
    "def add_accel_decel_alert(df: pl.DataFrame, interval: int) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Add acceleration/deceleration alerts based on EMA relationships and velocity status history.\n",
    "    \"\"\"\n",
    "    window_dict = {\n",
    "        1: 28, 3: 20, 5: 20, 8: 14, 13: 14\n",
    "    }\n",
    "    obs_window = window_dict.get(interval, 7)\n",
    "    \n",
    "    # First get velocity status\n",
    "    df = add_velocity_alert(df)\n",
    "    \n",
    "    # Count velocity statuses in the observation window\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"velocity_status\").map_elements(\n",
    "            lambda s: 1 if s in [\"velocity_loss\", \"velocity_weak\", \"velocity_negotiating\"] else 0,\n",
    "            return_dtype=pl.Int32\n",
    "        ).alias(\"loss_flag\"),\n",
    "        pl.col(\"velocity_status\").map_elements(\n",
    "            lambda s: 1 if s == \"velocity_maintained\" else 0,\n",
    "            return_dtype=pl.Int32\n",
    "        ).alias(\"maintain_flag\")\n",
    "    ])\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        pl.col(\"loss_flag\").rolling_sum(window_size=obs_window).alias(\"count_velocity_loss\"),\n",
    "        pl.col(\"maintain_flag\").rolling_sum(window_size=obs_window).alias(\"count_velocity_maintained\")\n",
    "    ])\n",
    "    \n",
    "    # Add acceleration/deceleration signals\n",
    "    df = df.with_columns([\n",
    "        pl.when(\n",
    "            (pl.max_horizontal(\"EMA_144\", \"EMA_169\") <= pl.max_horizontal(\"EMA_8\", \"EMA_13\")) &\n",
    "            (pl.col(\"open\") < pl.col(\"close\")) &\n",
    "            (pl.col(\"count_velocity_loss\") > pl.col(\"count_velocity_maintained\"))\n",
    "        ).then(pl.lit(\"accelerated\"))\n",
    "        .when(\n",
    "            (pl.col(\"close\") < pl.min_horizontal(\"EMA_8\", \"EMA_13\")) &\n",
    "            (pl.col(\"count_velocity_maintained\") < pl.col(\"count_velocity_loss\"))\n",
    "        ).then(pl.lit(\"decelerated\"))\n",
    "        .otherwise(None).alias(\"momentum_signal\")\n",
    "    ])\n",
    "    \n",
    "    # Create alert\n",
    "    momentum_alerts = df.filter(pl.col(\"momentum_signal\").is_not_null())\n",
    "    momentum_alerts = momentum_alerts.with_columns([\n",
    "        pl.lit(\"momentum_alert\").alias(\"alert_type\"),\n",
    "        pl.col(\"momentum_signal\").alias(\"signal\"),\n",
    "        pl.lit(interval).alias(\"interval\")\n",
    "    ])\n",
    "    \n",
    "    return momentum_alerts.select(\"symbol\", \"date\", \"interval\", \"alert_type\", \"signal\")\n",
    "\n",
    "# Step 3: Add EMA touch alerts\n",
    "def add_ema_touch_alert(df: pl.DataFrame, interval: int) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Add alerts for when price touches or comes close to important EMAs.\n",
    "    \"\"\"\n",
    "    tolerance_dict = {\n",
    "        1: 0.002, 3: 0.02, 5: 0.05, 8: 0.07, 13: 0.1\n",
    "    }\n",
    "    tolerance = tolerance_dict.get(interval, 0.02)\n",
    "    \n",
    "    # Calculate tolerance bands around EMAs\n",
    "    df = df.with_columns([\n",
    "        pl.min_horizontal(\n",
    "            pl.col(\"EMA_144\"), pl.col(\"EMA_169\")\n",
    "        ).fill_null(pl.col(\"EMA_13\")).alias(\"long_term_min\"),\n",
    "        \n",
    "        pl.max_horizontal(\n",
    "            pl.col(\"EMA_144\"), pl.col(\"EMA_169\")\n",
    "        ).fill_null(pl.col(\"EMA_13\")).alias(\"long_term_max\"),\n",
    "        \n",
    "        pl.min_horizontal(\n",
    "            pl.col(\"EMA_8\"), pl.col(\"EMA_13\")\n",
    "        ).alias(\"short_term_min\"),\n",
    "        \n",
    "        pl.max_horizontal(\n",
    "            pl.col(\"EMA_8\"), pl.col(\"EMA_13\")\n",
    "        ).alias(\"short_term_max\")\n",
    "    ])\n",
    "    \n",
    "    # Calculate tolerance bands\n",
    "    df = df.with_columns([\n",
    "        (pl.col(\"long_term_min\") * (1 - tolerance)).alias(\"lower_bound\"),\n",
    "        (pl.col(\"long_term_max\") * (1 + tolerance)).alias(\"upper_bound\")\n",
    "    ])\n",
    "    \n",
    "    # Detect touches\n",
    "    df = df.with_columns([\n",
    "        pl.when(\n",
    "            ((pl.col(\"low\") <= pl.col(\"upper_bound\")) & (pl.col(\"low\") >= pl.col(\"lower_bound\"))) |\n",
    "            ((pl.col(\"EMA_13\") <= pl.col(\"upper_bound\")) & (pl.col(\"EMA_13\") >= pl.col(\"lower_bound\"))) |\n",
    "            ((pl.col(\"EMA_8\") <= pl.col(\"upper_bound\")) & (pl.col(\"EMA_8\") >= pl.col(\"lower_bound\")))\n",
    "        ).then(\n",
    "            pl.when(\n",
    "                (pl.col(\"short_term_min\") > pl.col(\"long_term_max\")) &\n",
    "                (pl.min_horizontal(pl.col(\"close\"), pl.col(\"open\")) > pl.col(\"long_term_min\"))\n",
    "            ).then(pl.lit(\"support\"))\n",
    "            .when(\n",
    "                (pl.col(\"short_term_max\") < pl.col(\"long_term_max\")) &\n",
    "                (pl.col(\"close\") < pl.col(\"long_term_max\"))\n",
    "            ).then(pl.lit(\"resistance\"))\n",
    "            .otherwise(pl.lit(\"neutral\"))\n",
    "        ).otherwise(None).alias(\"ema_touch_type\")\n",
    "    ])\n",
    "    \n",
    "    # Filter for touches and create alert\n",
    "    ema_touch_alerts = df.filter(pl.col(\"ema_touch_type\").is_not_null())\n",
    "    ema_touch_alerts = ema_touch_alerts.with_columns([\n",
    "        pl.lit(\"ema_touch\").alias(\"alert_type\"),\n",
    "        pl.col(\"ema_touch_type\").alias(\"signal\"),\n",
    "        pl.lit(interval).alias(\"interval\")\n",
    "    ])\n",
    "    \n",
    "    return ema_touch_alerts.select(\"symbol\", \"date\", \"interval\", \"alert_type\", \"signal\")\n",
    "\n",
    "# Step 4: Process all intervals and combine alerts\n",
    "all_alerts = []\n",
    "df_with_indicators = df_with_indicators.filter(pl.col(\"date\").dt.replace_time_zone(\"America/Edmonton\") >= pd.to_datetime(\"2020-01-01\").tz_localize(\"America/Edmonton\"))\n",
    "\n",
    "for interval in intervals:\n",
    "    df_interval = df_with_indicators.filter(pl.col(\"interval\") == interval)\n",
    "    \n",
    "    # Skip empty DataFrames\n",
    "    if df_interval.height == 0:\n",
    "        continue\n",
    "        \n",
    "    # Add velocity alerts\n",
    "    velocity_df = add_velocity_alert(df_interval)\n",
    "    velocity_alerts = velocity_df.with_columns([\n",
    "        pl.lit(\"velocity_alert\").alias(\"alert_type\"),\n",
    "        pl.col(\"velocity_status\").alias(\"signal\"),\n",
    "        pl.lit(interval).alias(\"interval\")\n",
    "    ]).select(\"symbol\", \"date\", \"interval\", \"alert_type\", \"signal\")\n",
    "    \n",
    "    # Add momentum alerts\n",
    "    momentum_alerts = add_accel_decel_alert(df_interval, interval)\n",
    "    \n",
    "    # Add EMA touch alerts\n",
    "    ema_touch_alerts = add_ema_touch_alert(df_interval, interval)\n",
    "    \n",
    "    # Combine all alerts for this interval\n",
    "    all_alerts.extend([\n",
    "        velocity_alerts,\n",
    "        momentum_alerts,\n",
    "        ema_touch_alerts\n",
    "    ])\n",
    "\n",
    "# Step 5: Combine all alerts into final DataFrame\n",
    "if all_alerts:\n",
    "    alert_df = pl.concat(all_alerts)\n",
    "else:\n",
    "    # Return empty DataFrame with correct schema if no alerts\n",
    "    alert_df = pl.DataFrame({\n",
    "        \"symbol\": [],\n",
    "        \"date\": [],\n",
    "        \"interval\": [],\n",
    "        \"alert_type\": [],\n",
    "        \"signal\": []\n",
    "    })\n",
    "\n",
    "# Print results for verification\n",
    "print(\"\\nFirst 10 rows of alerts:\")\n",
    "print(alert_df.head(10))\n",
    "\n",
    "# Filter specific alert types for analysis\n",
    "momentum_df = alert_df.filter(pl.col(\"alert_type\") == \"momentum_alert\")\n",
    "ema_df = alert_df.filter(pl.col(\"alert_type\") == \"ema_touch\")\n",
    "\n",
    "print(\"\\nFirst 5 momentum alerts:\")\n",
    "print(momentum_df.head())\n",
    "\n",
    "print(\"\\nFirst 5 EMA touch alerts:\")\n",
    "print(ema_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize interval weights\n",
    "distinct_intervals = alert_df.get_column(\"interval\").unique().sort()\n",
    "interval_weights = {interval: weight for weight, interval in enumerate(distinct_intervals, 1)}\n",
    "print(\"Interval weights:\", interval_weights)\n",
    "\n",
    "# Step 2: Process momentum alerts for micro intervals\n",
    "momentum_data = alert_df.filter(pl.col(\"alert_type\") == \"momentum_alert\")\n",
    "momentum_results = momentum_data.group_by([\"symbol\", \"interval\"]).agg([\n",
    "    pl.when(pl.col(\"signal\") == \"accelerated\").then(1).otherwise(0).sum().alias(\"momentum_alert_accelerated\"),\n",
    "    pl.when(pl.col(\"signal\") == \"decelerated\").then(1).otherwise(0).sum().alias(\"momentum_alert_decelerated\")\n",
    "])\n",
    "print(\"\\nMomentum results:\")\n",
    "print(momentum_results.head())\n",
    "\n",
    "# Step 3: Process EMA touch alerts\n",
    "ema_data = alert_df.filter(pl.col(\"alert_type\") == \"ema_touch\")\n",
    "ema_results = ema_data.group_by([\"symbol\", \"interval\"]).agg([\n",
    "    pl.when(pl.col(\"signal\") == \"resistance\").then(1).otherwise(0).sum().alias(\"touch_type_resistance\"),\n",
    "    pl.when(pl.col(\"signal\") == \"support\").then(1).otherwise(0).sum().alias(\"touch_type_support\"),\n",
    "    pl.len().alias(\"count\")\n",
    "])\n",
    "print(\"\\nEMA touch results:\")\n",
    "print(ema_results.head())\n",
    "\n",
    "# Step 4: Join momentum and EMA results\n",
    "micro_results = momentum_results.join(\n",
    "    ema_results,\n",
    "    on=[\"symbol\", \"interval\"],\n",
    "    how=\"full\"\n",
    ").fill_null(0)\n",
    "print(\"\\nCombined micro results:\")\n",
    "print(micro_results.head())\n",
    "\n",
    "# Step 5: Apply interval weighting to micro results\n",
    "micro_results = micro_results.with_columns([\n",
    "    pl.col(\"interval\").map_elements(lambda x: interval_weights.get(x, 0), return_dtype=pl.Int64).alias(\"interval_weight\")\n",
    "])\n",
    "print(\"\\nMicro results with weights:\")\n",
    "print(micro_results.head())\n",
    "\n",
    "# Step 6: Calculate weighted values for micro results\n",
    "micro_results = micro_results.with_columns([\n",
    "    (pl.col(\"momentum_alert_accelerated\") * pl.col(\"interval_weight\")).alias(\"weighted_momentum_alert_accelerated\"),\n",
    "    (pl.col(\"momentum_alert_decelerated\") * pl.col(\"interval_weight\")).alias(\"weighted_momentum_alert_decelerated\"),\n",
    "    (pl.col(\"touch_type_resistance\") * pl.col(\"interval_weight\")).alias(\"weighted_touch_type_resistance\"),\n",
    "    (pl.col(\"touch_type_support\") * pl.col(\"interval_weight\")).alias(\"weighted_touch_type_support\")\n",
    "])\n",
    "print(\"\\nMicro results with weighted values:\")\n",
    "print(micro_results.head())\n",
    "\n",
    "# Step 7: Process velocity alerts for macro intervals\n",
    "velocity_data = alert_df.filter(pl.col(\"alert_type\") == \"velocity_alert\")\n",
    "macro_results = velocity_data.group_by([\"symbol\", \"interval\"]).agg([\n",
    "    pl.when(pl.col(\"signal\") == \"velocity_maintained\").then(1).otherwise(0).sum().alias(\"velocity_maintained\"),\n",
    "    pl.when(pl.col(\"signal\") == \"velocity_weak\").then(1).otherwise(0).sum().alias(\"velocity_weak\"),\n",
    "    pl.when(pl.col(\"signal\") == \"velocity_loss\").then(1).otherwise(0).sum().alias(\"velocity_loss\")\n",
    "])\n",
    "print(\"\\nMacro results:\")\n",
    "print(macro_results.head())\n",
    "\n",
    "# Step 8: Apply interval weighting to macro results\n",
    "macro_results = macro_results.with_columns([\n",
    "    pl.col(\"interval\").map_elements(lambda x: interval_weights.get(x, 0), return_dtype=pl.Int64).alias(\"interval_weight\")\n",
    "])\n",
    "print(\"\\nMacro results with weights:\")\n",
    "print(macro_results.head())\n",
    "\n",
    "# Step 9: Calculate weighted values for macro results\n",
    "macro_results = macro_results.with_columns([\n",
    "    (pl.col(\"velocity_maintained\") * pl.col(\"interval_weight\")).alias(\"weighted_velocity_maintained\"),\n",
    "    (pl.col(\"velocity_weak\") * pl.col(\"interval_weight\")).alias(\"weighted_velocity_weak\"),\n",
    "    (pl.col(\"velocity_loss\") * pl.col(\"interval_weight\")).alias(\"weighted_velocity_loss\")\n",
    "])\n",
    "print(\"\\nMacro results with weighted values:\")\n",
    "print(macro_results.head())\n",
    "\n",
    "# Step 10: Filter for specific stock categories\n",
    "short_acc_equ = micro_results.filter(\n",
    "    (pl.col(\"weighted_momentum_alert_accelerated\") > 1) &\n",
    "    (pl.col(\"weighted_momentum_alert_decelerated\") < 1) &\n",
    "    (pl.col(\"interval\") <= 3)\n",
    ").get_column(\"symbol\")\n",
    "\n",
    "lng_acc_equ = micro_results.filter(\n",
    "    (pl.col(\"weighted_momentum_alert_accelerated\") > 1) &\n",
    "    (pl.col(\"weighted_momentum_alert_decelerated\") < 1) &\n",
    "    (pl.col(\"interval\") == 5)\n",
    ").get_column(\"symbol\")\n",
    "\n",
    "lng_main_acc_equ = micro_results.filter(\n",
    "    (pl.col(\"weighted_touch_type_support\") > 1) &\n",
    "    (pl.col(\"weighted_touch_type_resistance\") < 1) &\n",
    "    (pl.col(\"count\") >= 1) &\n",
    "    (pl.col(\"interval\") == 5)\n",
    ").get_column(\"symbol\")\n",
    "\n",
    "maintained_stocks = macro_results.filter(\n",
    "    (pl.col(\"weighted_velocity_maintained\") > 0) &\n",
    "    (pl.col(\"weighted_velocity_weak\") == 0) &\n",
    "    (pl.col(\"weighted_velocity_loss\") == 0) &\n",
    "    (pl.col(\"interval\") >= 8)\n",
    ").get_column(\"symbol\")\n",
    "\n",
    "print(\"\\nShort accelerating stocks:\", short_acc_equ.to_list())\n",
    "print(\"\\nLong accelerating stocks:\", lng_acc_equ.to_list())\n",
    "print(\"\\nLong accumulating stocks:\", lng_main_acc_equ.to_list())\n",
    "print(\"\\nVelocity maintained stocks:\", maintained_stocks.to_list())\n",
    "\n",
    "# Step 11: Group results by date and create final DataFrame\n",
    "grouped_data = alert_df.group_by(\"date\")\n",
    "results = []\n",
    "\n",
    "for date, group in grouped_data:\n",
    "    micro_data = group.filter(pl.col(\"interval\") <= 5)\n",
    "    macro_data = group.filter(pl.col(\"interval\") >= 8)\n",
    "    \n",
    "    date_results = {\n",
    "        \"date\": date,\n",
    "        \"accelerating\": short_acc_equ.to_list(),\n",
    "        \"long_accelerating\": lng_acc_equ.to_list(),\n",
    "        \"long_accumulating\": lng_main_acc_equ.to_list(),\n",
    "        \"velocity_maintained\": maintained_stocks.to_list()\n",
    "    }\n",
    "    results.append(date_results)\n",
    "\n",
    "final_results = pl.DataFrame(results)\n",
    "print(\"\\nFinal results:\")\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, user: str, password: str, intervals: List[int]):\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.con = duckdb.connect()\n",
    "        self.intervals = intervals\n",
    "\n",
    "    def _get_resampled_data(self, interval: int) -> pd.DataFrame:\n",
    "        query = f\"\"\"\n",
    "        WITH raw_data AS (\n",
    "            SELECT * FROM postgres_scan(\n",
    "                'host=localhost port=5432 user={self.user} password={self.password} dbname=condvest',\n",
    "                'public', 'raw'\n",
    "            )\n",
    "        ),\n",
    "        ranked AS (\n",
    "            SELECT *,\n",
    "                row_number() OVER (PARTITION BY symbol ORDER BY date) as rn\n",
    "            FROM raw_data\n",
    "        ),\n",
    "        grouped AS (\n",
    "            SELECT *,\n",
    "                (rn - 1) / {interval} as group_id\n",
    "            FROM ranked\n",
    "        )\n",
    "        SELECT \n",
    "            symbol,\n",
    "            min(date) as date,\n",
    "            first(open) as open,\n",
    "            max(high) as high,\n",
    "            min(low) as low,\n",
    "            last(close) as close,\n",
    "            sum(volume) as volume,\n",
    "            '{interval}'::INT as interval\n",
    "        FROM grouped\n",
    "        GROUP BY symbol, group_id\n",
    "        ORDER BY symbol, date;\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.con.sql(query).df()\n",
    "\n",
    "    def load_data(self) -> pl.DataFrame:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load and resample data for each interval\n",
    "        combined_results = []\n",
    "        for interval in self.intervals:\n",
    "            interval_start = time.time()\n",
    "            df = self._get_resampled_data(interval)\n",
    "            print(f\"DuckDB query execution time for interval {interval}: {time.time() - interval_start:.2f} seconds\")\n",
    "            combined_results.append(df)\n",
    "            print(f\"Duck to dataframe conversion time: {time.time() - interval_start:.2f} seconds\")\n",
    "\n",
    "        # Convert to Polars DataFrame\n",
    "        pl_resampled_df = pl.from_pandas(pd.concat(combined_results))\n",
    "        print(f\"Pandas to Polars conversion time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        return pl_resampled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data Class\n",
    "data_loader = DataLoader(user=user, password=password, intervals=intervals)\n",
    "df = data_loader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator Calculator Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndicatorCalculator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def add_indicators(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        df = df.sort(\"date\")\n",
    "        \n",
    "        # Compute EMAs\n",
    "        df = df.with_columns([\n",
    "            pl.col(\"close\").ewm_mean(span=8).alias(\"EMA_8\"),\n",
    "            pl.col(\"close\").ewm_mean(span=13).alias(\"EMA_13\"),\n",
    "            pl.col(\"close\").ewm_mean(span=21).alias(\"EMA_21\"),\n",
    "            pl.col(\"close\").ewm_mean(span=144).alias(\"EMA_144\"),\n",
    "            pl.col(\"close\").ewm_mean(span=169).alias(\"EMA_169\"),\n",
    "            pl.col(\"close\").ewm_mean(span=55).alias(\"EMA_55\"),\n",
    "            pl.col(\"close\").ewm_mean(span=89).alias(\"EMA_89\"),\n",
    "        ])\n",
    "        \n",
    "        # Compute MACD\n",
    "        df = df.with_columns([\n",
    "            (pl.col(\"EMA_13\") - pl.col(\"EMA_21\")).alias(\"macd_fast\"),\n",
    "            (pl.col(\"EMA_55\") - pl.col(\"EMA_89\")).alias(\"macd_slow\"),\n",
    "        ])\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def calculate_indicators(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Add indicators\n",
    "        df_with_indicators = df.group_by(\"symbol\", maintain_order=True).map_groups(self.add_indicators)\n",
    "        print(f\"Indicator calculation time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        return df_with_indicators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate indicators\n",
    "indicator_calculator = IndicatorCalculator()\n",
    "results = indicator_calculator.calculate_indicators(df)\n",
    "print(\"\\nFirst 10 rows of result:\")\n",
    "print(results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Alerts Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "class TrendAlertProcessor:\n",
    "    \"\"\"\n",
    "    TrendAlertProcessor using Polars for efficient processing of financial time series data.\n",
    "    Incorporates advanced trend detection algorithms from the dictionary-based implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pl.DataFrame, intervals: List[int]):\n",
    "        self.df = df\n",
    "        self.intervals = intervals\n",
    "        self.rolling_window = 50\n",
    "    \n",
    "    def _add_velocity_alert(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Add velocity alerts based on the relationship between price and various EMAs.\n",
    "        Similar to velocity_alert_dict in the original implementation.\n",
    "        \"\"\"\n",
    "        # Add velocity status\n",
    "        df = df.with_columns([\n",
    "            pl.when(\n",
    "                (pl.col(\"close\") > pl.col(\"open\")) & \n",
    "                (pl.col(\"close\") > pl.max_horizontal(\"EMA_8\", \"EMA_13\")) & \n",
    "                (pl.col(\"close\") > pl.max_horizontal(\"EMA_144\", \"EMA_169\")) &\n",
    "                (pl.min_horizontal(\"EMA_8\", \"EMA_13\") > pl.max_horizontal(\"EMA_144\", \"EMA_169\"))\n",
    "            ).then(pl.lit(\"velocity_maintained\"))\n",
    "            .when(\n",
    "                (pl.col(\"close\") < pl.col(\"EMA_13\")) & \n",
    "                (pl.col(\"close\") > pl.col(\"EMA_169\"))\n",
    "            ).then(pl.lit(\"velocity_weak\"))\n",
    "            .when(\n",
    "                (pl.col(\"close\") < pl.col(\"EMA_13\")) & \n",
    "                (pl.col(\"close\") < pl.col(\"EMA_169\"))\n",
    "            ).then(pl.lit(\"velocity_loss\"))\n",
    "            .otherwise(pl.lit(\"velocity_negotiating\"))\n",
    "            .alias(\"velocity_status\")\n",
    "        ])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _add_accel_decel_alert(self, df: pl.DataFrame, interval: int) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Add acceleration/deceleration alerts based on EMA relationships and velocity status history.\n",
    "        \"\"\"\n",
    "        window_dict = {\n",
    "            1: 28, 3: 20, 5: 20, 8: 14, 13: 14\n",
    "        }\n",
    "        obs_window = window_dict.get(interval, 7)\n",
    "        \n",
    "        # First get velocity status\n",
    "        df = self._add_velocity_alert(df)\n",
    "        \n",
    "        # Count velocity statuses in the observation window\n",
    "        df = df.with_columns([\n",
    "            pl.col(\"velocity_status\").map_elements(\n",
    "                lambda s: 1 if s in [\"velocity_loss\", \"velocity_weak\", \"velocity_negotiating\"] else 0,\n",
    "                return_dtype=pl.Int32\n",
    "            ).alias(\"loss_flag\"),\n",
    "            pl.col(\"velocity_status\").map_elements(\n",
    "                lambda s: 1 if s == \"velocity_maintained\" else 0,\n",
    "                return_dtype=pl.Int32\n",
    "            ).alias(\"maintain_flag\")\n",
    "        ])\n",
    "        \n",
    "        df = df.with_columns([\n",
    "            pl.col(\"loss_flag\").rolling_sum(window_size=obs_window).alias(\"count_velocity_loss\"),\n",
    "            pl.col(\"maintain_flag\").rolling_sum(window_size=obs_window).alias(\"count_velocity_maintained\")\n",
    "        ])\n",
    "        \n",
    "        # Add acceleration/deceleration signals\n",
    "        df = df.with_columns([\n",
    "            pl.when(\n",
    "                (pl.max_horizontal(\"EMA_144\", \"EMA_169\") <= pl.max_horizontal(\"EMA_8\", \"EMA_13\")) &\n",
    "                (pl.col(\"open\") < pl.col(\"close\")) &\n",
    "                (pl.col(\"count_velocity_loss\") > pl.col(\"count_velocity_maintained\"))\n",
    "            ).then(pl.lit(\"accelerated\"))\n",
    "            .when(\n",
    "                (pl.col(\"close\") < pl.min_horizontal(\"EMA_8\", \"EMA_13\")) &\n",
    "                (pl.col(\"count_velocity_maintained\") < pl.col(\"count_velocity_loss\"))\n",
    "            ).then(pl.lit(\"decelerated\"))\n",
    "            .otherwise(None).alias(\"momentum_signal\")\n",
    "        ])\n",
    "        \n",
    "        # Create alert\n",
    "        momentum_alerts = df.filter(pl.col(\"momentum_signal\").is_not_null())\n",
    "        momentum_alerts = momentum_alerts.with_columns([\n",
    "            pl.lit(\"momentum_alert\").alias(\"alert_type\"),\n",
    "            pl.col(\"momentum_signal\").alias(\"signal\"),\n",
    "            pl.lit(interval).alias(\"interval\")\n",
    "        ])\n",
    "        \n",
    "        return momentum_alerts.select(\"symbol\", \"date\", \"interval\", \"alert_type\", \"signal\")\n",
    "    \n",
    "    def _add_ema_touch_alert(self, df: pl.DataFrame, interval: int) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Add alerts for when price touches or comes close to important EMAs.\n",
    "        \"\"\"\n",
    "        tolerance_dict = {\n",
    "            1: 0.002, 3: 0.02, 5: 0.05, 8: 0.07, 13: 0.1\n",
    "        }\n",
    "        tolerance = tolerance_dict.get(interval, 0.02)\n",
    "        \n",
    "        # Calculate tolerance bands around EMAs\n",
    "        df = df.with_columns([\n",
    "            pl.min_horizontal(\n",
    "                pl.col(\"EMA_144\"), pl.col(\"EMA_169\")\n",
    "            ).fill_null(pl.col(\"EMA_13\")).alias(\"long_term_min\"),\n",
    "            \n",
    "            pl.max_horizontal(\n",
    "                pl.col(\"EMA_144\"), pl.col(\"EMA_169\")\n",
    "            ).fill_null(pl.col(\"EMA_13\")).alias(\"long_term_max\"),\n",
    "            \n",
    "            pl.min_horizontal(\n",
    "                pl.col(\"EMA_8\"), pl.col(\"EMA_13\")\n",
    "            ).alias(\"short_term_min\"),\n",
    "            \n",
    "            pl.max_horizontal(\n",
    "                pl.col(\"EMA_8\"), pl.col(\"EMA_13\")\n",
    "            ).alias(\"short_term_max\")\n",
    "        ])\n",
    "        \n",
    "        # Calculate tolerance bands\n",
    "        df = df.with_columns([\n",
    "            (pl.col(\"long_term_min\") * (1 - tolerance)).alias(\"lower_bound\"),\n",
    "            (pl.col(\"long_term_max\") * (1 + tolerance)).alias(\"upper_bound\")\n",
    "        ])\n",
    "        \n",
    "        # Detect touches\n",
    "        df = df.with_columns([\n",
    "            pl.when(\n",
    "                ((pl.col(\"low\") <= pl.col(\"upper_bound\")) & (pl.col(\"low\") >= pl.col(\"lower_bound\"))) |\n",
    "                ((pl.col(\"EMA_13\") <= pl.col(\"upper_bound\")) & (pl.col(\"EMA_13\") >= pl.col(\"lower_bound\"))) |\n",
    "                ((pl.col(\"EMA_8\") <= pl.col(\"upper_bound\")) & (pl.col(\"EMA_8\") >= pl.col(\"lower_bound\")))\n",
    "            ).then(\n",
    "                pl.when(\n",
    "                    (pl.col(\"short_term_min\") > pl.col(\"long_term_max\")) &\n",
    "                    (pl.min_horizontal(pl.col(\"close\"), pl.col(\"open\")) > pl.col(\"long_term_min\"))\n",
    "                ).then(pl.lit(\"support\"))\n",
    "                .when(\n",
    "                    (pl.col(\"short_term_max\") < pl.col(\"long_term_max\")) &\n",
    "                    (pl.col(\"close\") < pl.col(\"long_term_max\"))\n",
    "                ).then(pl.lit(\"resistance\"))\n",
    "                .otherwise(pl.lit(\"neutral\"))\n",
    "            ).otherwise(None).alias(\"ema_touch_type\")\n",
    "        ])\n",
    "        \n",
    "        # Filter for touches and create alert\n",
    "        ema_touch_alerts = df.filter(pl.col(\"ema_touch_type\").is_not_null())\n",
    "        ema_touch_alerts = ema_touch_alerts.with_columns([\n",
    "            pl.lit(\"ema_touch\").alias(\"alert_type\"),\n",
    "            pl.col(\"ema_touch_type\").alias(\"signal\"),\n",
    "            pl.lit(interval).alias(\"interval\")\n",
    "        ])\n",
    "        \n",
    "        return ema_touch_alerts.select(\"symbol\", \"date\", \"interval\", \"alert_type\", \"signal\")\n",
    "    \n",
    "    def apply(self) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply all alert detection algorithms and return a combined DataFrame of alerts.\n",
    "        \"\"\"\n",
    "        all_alerts = []\n",
    "        \n",
    "        for interval in self.intervals:\n",
    "            df_interval = self.df.filter(pl.col(\"interval\") == interval)\n",
    "            \n",
    "            # No empty DataFrames\n",
    "            if df_interval.height == 0:\n",
    "                continue\n",
    "                \n",
    "            # Add velocity alerts\n",
    "            velocity_df = self._add_velocity_alert(df_interval)\n",
    "            velocity_alerts = velocity_df.with_columns([\n",
    "                pl.lit(\"velocity_alert\").alias(\"alert_type\"),\n",
    "                pl.col(\"velocity_status\").alias(\"signal\"),\n",
    "                pl.lit(interval).alias(\"interval\")\n",
    "            ]).select(\"symbol\", \"date\", \"interval\", \"alert_type\", \"signal\")\n",
    "            \n",
    "            # Add momentum alerts\n",
    "            momentum_alerts = self._add_accel_decel_alert(df_interval, interval)\n",
    "            \n",
    "            # Add EMA touch alerts\n",
    "            ema_touch_alerts = self._add_ema_touch_alert(df_interval, interval)\n",
    "            \n",
    "            # Combine all alerts for this interval\n",
    "            all_alerts.extend([\n",
    "                velocity_alerts,\n",
    "                momentum_alerts,\n",
    "                ema_touch_alerts\n",
    "            ])\n",
    "        \n",
    "        # Combine all alerts into a single DataFrame\n",
    "        if all_alerts:\n",
    "            return pl.concat(all_alerts)\n",
    "        else:\n",
    "            # Return empty DataFrame with correct schema if no alerts\n",
    "            return pl.DataFrame({\n",
    "                \"symbol\": [],\n",
    "                \"date\": [],\n",
    "                \"interval\": [],\n",
    "                \"alert_type\": [],\n",
    "                \"signal\": []\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Signal Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockCandidatesProcessor:\n",
    "    \"\"\"\n",
    "    A class to process and analyze stock candidates using Polars for efficient data processing.\n",
    "    Analyzes stocks based on various criteria and alerts from different time intervals.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pl.DataFrame):\n",
    "        self.df = df\n",
    "        self.interval_weights = None\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize interval weights based on unique intervals in the data.\"\"\"\n",
    "        distinct_intervals = self.df.get_column(\"interval\").unique().sort()\n",
    "        self.interval_weights = {interval: weight for weight, interval in enumerate(distinct_intervals, 1)}\n",
    "        \n",
    "    def _evaluate_micro_interval_stocks(self, data: pl.DataFrame) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate stocks based on micro-interval criteria (intervals <= 5).\n",
    "        Analyzes acceleration and accumulation patterns.\n",
    "        \"\"\"\n",
    "        # Process momentum alerts\n",
    "        momentum_data = data.filter(pl.col(\"alert_type\") == \"momentum_alert\")\n",
    "        momentum_results = momentum_data.group_by([\"symbol\", \"interval\"]).agg([\n",
    "            pl.when(pl.col(\"signal\") == \"accelerated\").then(1).otherwise(0).sum().alias(\"momentum_alert_accelerated\"),\n",
    "            pl.when(pl.col(\"signal\") == \"decelerated\").then(1).otherwise(0).sum().alias(\"momentum_alert_decelerated\")\n",
    "        ])\n",
    "        \n",
    "        # Process EMA touch alerts\n",
    "        ema_data = data.filter(pl.col(\"alert_type\") == \"ema_touch\")\n",
    "        ema_results = ema_data.group_by([\"symbol\", \"interval\"]).agg([\n",
    "            pl.when(pl.col(\"signal\") == \"resistance\").then(1).otherwise(0).sum().alias(\"touch_type_resistance\"),\n",
    "            pl.when(pl.col(\"signal\") == \"support\").then(1).otherwise(0).sum().alias(\"touch_type_support\"),\n",
    "            pl.len().alias(\"count\")\n",
    "        ])\n",
    "        \n",
    "        # Join the results\n",
    "        results = momentum_results.join(\n",
    "            ema_results,\n",
    "            on=[\"symbol\", \"interval\"],\n",
    "            how=\"full\"\n",
    "        ).fill_null(0)\n",
    "        \n",
    "        # Apply interval weighting\n",
    "        results = results.with_columns([\n",
    "            pl.col(\"interval\").map_elements(lambda x: self.interval_weights.get(x, 0), return_dtype=pl.Int64).alias(\"interval_weight\")\n",
    "        ])\n",
    "        \n",
    "        # Calculate weighted values\n",
    "        results = results.with_columns([\n",
    "            (pl.col(\"momentum_alert_accelerated\") * pl.col(\"interval_weight\")).alias(\"weighted_momentum_alert_accelerated\"),\n",
    "            (pl.col(\"momentum_alert_decelerated\") * pl.col(\"interval_weight\")).alias(\"weighted_momentum_alert_decelerated\"),\n",
    "            (pl.col(\"touch_type_resistance\") * pl.col(\"interval_weight\")).alias(\"weighted_touch_type_resistance\"),\n",
    "            (pl.col(\"touch_type_support\") * pl.col(\"interval_weight\")).alias(\"weighted_touch_type_support\")\n",
    "        ])\n",
    "        \n",
    "        # Filter for accelerating stocks\n",
    "        short_acc_equ = results.filter(\n",
    "            (pl.col(\"weighted_momentum_alert_accelerated\") > 1) &\n",
    "            (pl.col(\"weighted_momentum_alert_decelerated\") < 1) &\n",
    "            (pl.col(\"interval\") <= 3)\n",
    "        ).get_column(\"symbol\")\n",
    "        \n",
    "        lng_acc_equ = results.filter(\n",
    "            (pl.col(\"weighted_momentum_alert_accelerated\") > 1) &\n",
    "            (pl.col(\"weighted_momentum_alert_decelerated\") < 1) &\n",
    "            (pl.col(\"interval\") == 5)\n",
    "        ).get_column(\"symbol\")\n",
    "        \n",
    "        lng_main_acc_equ = results.filter(\n",
    "            (pl.col(\"weighted_touch_type_support\") > 1) &\n",
    "            (pl.col(\"weighted_touch_type_resistance\") < 1) &\n",
    "            (pl.col(\"count\") >= 1) &\n",
    "            (pl.col(\"interval\") == 5)\n",
    "        ).get_column(\"symbol\")\n",
    "        \n",
    "        return {\n",
    "            \"accelerating\": short_acc_equ.to_list(),\n",
    "            \"long_accelerating\": lng_acc_equ.to_list(),\n",
    "            \"long_accumulating\": lng_main_acc_equ.to_list()\n",
    "        }\n",
    "\n",
    "    def _evaluate_macro_interval_stocks(self, data: pl.DataFrame) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate stocks based on macro-interval criteria (intervals >= 8).\n",
    "        Analyzes velocity maintenance patterns.\n",
    "        \"\"\"\n",
    "        # Process velocity alerts\n",
    "        velocity_data = data.filter(pl.col(\"alert_type\") == \"velocity_alert\")\n",
    "        results = velocity_data.group_by([\"symbol\", \"interval\"]).agg([\n",
    "            pl.when(pl.col(\"signal\") == \"velocity_maintained\").then(1).otherwise(0).sum().alias(\"velocity_maintained\"),\n",
    "            pl.when(pl.col(\"signal\") == \"velocity_weak\").then(1).otherwise(0).sum().alias(\"velocity_weak\"),\n",
    "            pl.when(pl.col(\"signal\") == \"velocity_loss\").then(1).otherwise(0).sum().alias(\"velocity_loss\")\n",
    "        ])\n",
    "        \n",
    "        # Apply interval weighting\n",
    "        results = results.with_columns([\n",
    "        pl.col(\"interval\").map_elements(lambda x: self.interval_weights.get(x, 0), return_dtype=pl.Int64).alias(\"interval_weight\")\n",
    "        ])\n",
    "        \n",
    "        # Calculate weighted values\n",
    "        results = results.with_columns([\n",
    "            (pl.col(\"velocity_maintained\") * pl.col(\"interval_weight\")).alias(\"weighted_velocity_maintained\"),\n",
    "            (pl.col(\"velocity_weak\") * pl.col(\"interval_weight\")).alias(\"weighted_velocity_weak\"),\n",
    "            (pl.col(\"velocity_loss\") * pl.col(\"interval_weight\")).alias(\"weighted_velocity_loss\")\n",
    "        ])\n",
    "        \n",
    "        # Filter for maintained velocity stocks\n",
    "        maintained_stocks = results.filter(\n",
    "            (pl.col(\"weighted_velocity_maintained\") > 0) &\n",
    "            (pl.col(\"weighted_velocity_weak\") == 0) &\n",
    "            (pl.col(\"weighted_velocity_loss\") == 0) &\n",
    "            (pl.col(\"interval\") >= 8)\n",
    "        ).get_column(\"symbol\")\n",
    "        \n",
    "        return {\n",
    "            \"velocity_maintained\": maintained_stocks.to_list()\n",
    "        }\n",
    "\n",
    "    def process(self) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Process the data and generate stock candidates for each date.\n",
    "        Returns a DataFrame with the results.\n",
    "        \"\"\"\n",
    "        # Group data by date\n",
    "        grouped_data = self.df.group_by(\"date\")\n",
    "        \n",
    "        # Process candidates for each date\n",
    "        results = []\n",
    "        \n",
    "        for date, group in grouped_data:\n",
    "            # Process micro-interval data\n",
    "            micro_data = group.filter(pl.col(\"interval\") <= 5)\n",
    "            micro_results = self._evaluate_micro_interval_stocks(micro_data)\n",
    "            \n",
    "            # Process macro-interval data\n",
    "            macro_data = group.filter(pl.col(\"interval\") >= 8)\n",
    "            macro_results = self._evaluate_macro_interval_stocks(macro_data)\n",
    "            \n",
    "            # Combine results\n",
    "            combined_results = {**micro_results, **macro_results}\n",
    "            combined_results[\"date\"] = date\n",
    "            \n",
    "            results.append(combined_results)\n",
    "        \n",
    "        # Convert results to DataFrame\n",
    "        return pl.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completed OLAP Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and preprocess data\n",
    "data_loader = DataLoader(user=user, password=password)\n",
    "df = data_loader.load_data()\n",
    "\n",
    "# Step 2: Calculate indicators\n",
    "indicator_calculator = IndicatorCalculator()\n",
    "df_with_indicators = indicator_calculator.calculate_indicators(df)\n",
    "\n",
    "# Step 3: Add alerts\n",
    "df_with_indicators = df_with_indicators.filter(pl.col(\"date\").dt.replace_time_zone(\"America/Edmonton\") >= pd.to_datetime(\"2020-01-01\").tz_localize(\"America/Edmonton\"))\n",
    "trend_alert = TrendAlertProcessor(df_with_indicators, intervals=[1, 3, 5, 8, 13])\n",
    "alert_df = trend_alert.apply()\n",
    "\n",
    "# Step 4: Add signals\n",
    "stock_candidates = StockCandidatesProcessor(alert_df)\n",
    "results_df = stock_candidates.process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort(\"date\", descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast API Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampled Data Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api.py\n",
    "import duckdb\n",
    "from fastapi import FastAPI, Query\n",
    "from pydantic import BaseModel, RootModel\n",
    "from typing import List, Any\n",
    "\n",
    "app = FastAPI()\n",
    "con = duckdb.connect()  # in-process, zero-config\n",
    "\n",
    "# Optionally register your Postgres raw table via postgres_scan:\n",
    "con.execute(\"\"\"\n",
    "    INSTALL httpfs; LOAD httpfs;\n",
    "  /* or: INSTALL postgres_scanner; LOAD postgres_scanner; */\n",
    "  /* then you can do: */\n",
    "  /* CREATE VIEW raw AS SELECT * FROM postgres_scan('host=','public','raw'); */\n",
    "\"\"\")\n",
    "\n",
    "class Row(RootModel):\n",
    "    root: List[Any]\n",
    "\n",
    "class QueryResult(BaseModel):\n",
    "    columns: List[str]\n",
    "    rows: List[Row]\n",
    "\n",
    "@app.get(\"/resample\", response_model=QueryResult)\n",
    "def resample(\n",
    "    interval: int = Query(3, description=\"Resample interval in days\"),\n",
    "    limit: int = Query(50, description=\"Max rows back\")\n",
    "):\n",
    "    sql = f\"\"\"\n",
    "    WITH ranked AS (\n",
    "      SELECT *,\n",
    "        ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY date) AS rn\n",
    "        FROM raw\n",
    "    ),\n",
    "    grp AS (\n",
    "      SELECT *,\n",
    "        (rn - 1) / {interval} AS grp_id\n",
    "        FROM ranked\n",
    "    )\n",
    "    SELECT\n",
    "        symbol,\n",
    "        MIN(date)   AS date,\n",
    "        FIRST(open) AS open,\n",
    "        MAX(high)   AS high,\n",
    "        MIN(low)    AS low,\n",
    "        LAST(close) AS close,\n",
    "        SUM(volume) AS volume\n",
    "    FROM grp\n",
    "    GROUP BY symbol, grp_id\n",
    "    ORDER BY symbol, date DESC\n",
    "    LIMIT {limit};\n",
    "    \"\"\"\n",
    "    # execute and grab both column names + native Python lists of tuples\n",
    "    cur = con.execute(sql)\n",
    "    cols = [c[0] for c in cur.description]\n",
    "    data = cur.fetchall()\n",
    "    return QueryResult(columns=cols, rows=[Row(root=list(r)) for r in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
