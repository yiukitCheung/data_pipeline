{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae92f371",
   "metadata": {},
   "source": [
    "# ðŸ“Š Batch Layer Processing - Performance Benchmarks\n",
    "\n",
    "This notebook contains performance benchmarks for the batch layer data processing components:\n",
    "\n",
    "1. **ðŸ”„ Resampling Performance**: DuckDB vs PostgreSQL for Fibonacci resampling\n",
    "2. **ðŸ“‚ S3 Reading Performance**: DuckDB vs Polars for reading Parquet from S3\n",
    "3. **ðŸ—œï¸ Consolidation Performance**: Testing bronze layer data consolidation\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¦ Library Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e10b867b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries loaded\n",
      "ðŸ“ S3 Bucket: dev-condvest-datalake\n",
      "ðŸ“ AWS Region: ca-west-1\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from io import BytesIO\n",
    "from datetime import datetime, date, timedelta\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Data processing\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# AWS & Database\n",
    "import boto3\n",
    "import psycopg2\n",
    "\n",
    "# Parallel processing\n",
    "import concurrent.futures\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# ============================================\n",
    "# Global Configuration\n",
    "# ============================================\n",
    "S3_BUCKET = 'dev-condvest-datalake'\n",
    "S3_PREFIX = 'bronze/raw_ohlcv'\n",
    "AWS_REGION = 'ca-west-1'\n",
    "\n",
    "print(\"âœ… Libraries loaded\")\n",
    "print(f\"ðŸ“ S3 Bucket: {S3_BUCKET}\")\n",
    "print(f\"ðŸ“ AWS Region: {AWS_REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cc839c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ”„ Performance Benchmark 1: DuckDB vs PostgreSQL Resampling\n",
    "\n",
    "**Goal**: Compare resampling performance between DuckDB (S3) and PostgreSQL (RDS)\n",
    "\n",
    "**Test Scenarios**:\n",
    "1. Single symbol Fibonacci resampling (3-day interval)\n",
    "2. Query performance for filtered data (industry-based queries)\n",
    "3. Time/memory comparison\n",
    "\n",
    "**Expected Outcome**: DuckDB should be faster for OLAP-style analytics on large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a818626c",
   "metadata": {},
   "source": [
    "## ðŸ˜ PostgreSQL (RDS) Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a12e800",
   "metadata": {},
   "source": [
    "### Setup: RDS Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69051867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDS Crendtials\n",
    "RDS_HOST = os.environ.get('RDS_HOST')\n",
    "RDS_PORT = os.environ.get('RDS_PORT')\n",
    "RDS_DATABASE = os.environ.get('RDS_DATABASE')\n",
    "RDS_USER = os.environ.get('RDS_USER')\n",
    "RDS_PASSWORD = os.environ.get('RDS_PASSWORD')\n",
    "# RDS Postgres Connection\n",
    "try:\n",
    "    rds_conn = psycopg2.connect(\n",
    "        host=RDS_HOST,\n",
    "        port=RDS_PORT,\n",
    "        database=RDS_DATABASE,\n",
    "        user=RDS_USER,\n",
    "        password=RDS_PASSWORD,\n",
    "        sslmode='require'\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to RDS: {e}\")\n",
    "    raise\n",
    "\n",
    "# RDS Postgres Cursor\n",
    "rds_cursor = rds_conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0f9626c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public.batch_jobs\n",
      "public.data_ingestion_watermark\n",
      "public.data_retention_log\n",
      "public.raw_ohlcv\n",
      "public.raw_ohlcv_archive\n",
      "public.silver_13d\n",
      "public.silver_21d\n",
      "public.silver_34d\n",
      "public.silver_3d\n",
      "public.silver_5d\n",
      "public.silver_8d\n",
      "public.symbol_metadata\n"
     ]
    }
   ],
   "source": [
    "# Check available tables in the connected database\n",
    "try:\n",
    "    rds_cursor.execute(\"\"\"\n",
    "        SELECT table_schema, table_name \n",
    "        FROM information_schema.tables \n",
    "        WHERE table_type = 'BASE TABLE' AND table_schema NOT IN ('pg_catalog', 'information_schema')\n",
    "        ORDER BY table_schema, table_name\n",
    "    \"\"\")\n",
    "    tables = rds_cursor.fetchall()\n",
    "    for schema, table in tables:\n",
    "        print(f\"{schema}.{table}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching tables: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676dca73",
   "metadata": {},
   "source": [
    "### Test 1a: PostgreSQL Resampling (Single Symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3be7686e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling for AMD took 1.2799 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Resampling with Postgres for only symbol 'AMD' and timing the processing\n",
    "sql = f\"\"\"\n",
    "    WITH numbered AS (\n",
    "        SELECT\n",
    "            symbol,\n",
    "            DATE(timestamp) as date,\n",
    "            open as open,\n",
    "            high as high,\n",
    "            low as low,\n",
    "            close as close,\n",
    "            volume,\n",
    "            ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY DATE(timestamp)) AS rn\n",
    "        FROM raw_ohlcv\n",
    "        WHERE interval = '1d'\n",
    "            AND symbol in ('AMD')\n",
    "    ),\n",
    "    grp AS (\n",
    "        SELECT\n",
    "            symbol,\n",
    "            date,\n",
    "            open,\n",
    "            high,\n",
    "            low,\n",
    "            close,\n",
    "            volume,\n",
    "            (rn - 1) / 3 AS grp_id\n",
    "        FROM numbered\n",
    "    ),\n",
    "    aggregated AS (\n",
    "        SELECT\n",
    "            symbol,\n",
    "            grp_id,\n",
    "            MIN(date) AS start_date,\n",
    "            MAX(high) AS high,\n",
    "            MIN(low) AS low,\n",
    "            SUM(volume) AS volume,\n",
    "            (array_agg(open ORDER BY date))[1] AS open,\n",
    "            (array_agg(close ORDER BY date DESC))[1] AS close\n",
    "        FROM grp\n",
    "        GROUP BY symbol, grp_id\n",
    "        HAVING COUNT(*) = 3\n",
    "    )\n",
    "    SELECT\n",
    "        symbol,\n",
    "        start_date AS date,\n",
    "        open,\n",
    "        high,\n",
    "        low,\n",
    "        close,\n",
    "        volume\n",
    "    FROM aggregated\n",
    "    ORDER BY symbol, date\n",
    "\"\"\"\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    rds_cursor.execute(sql)\n",
    "    postgres_resampled_data = rds_cursor.fetchall()\n",
    "    end_time = time.time()\n",
    "    print(f\"Resampling for AMD took {end_time - start_time:.4f} seconds\")\n",
    "    postgres_resampled_data\n",
    "    postgres_resampling_time = end_time - start_time\n",
    "except Exception as e:\n",
    "    print(f\"Error resampling data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba864d8",
   "metadata": {},
   "source": [
    "### Test 1b: PostgreSQL Industry Query (Filtered Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc3b92f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching symbol from Semiconductor industry took in past 5 years took 136.3175 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    rds_cursor.execute(\n",
    "        '''\n",
    "        SELECT ohlcv.* \n",
    "        FROM raw_ohlcv ohlcv\n",
    "        JOIN symbol_metadata meta ON ohlcv.symbol = meta.symbol\n",
    "        WHERE meta.industry = 'SEMICONDUCTORS & RELATED DEVICES'\n",
    "            AND ohlcv.timestamp >= NOW() - INTERVAL '5 years'\n",
    "        ORDER BY ohlcv.symbol, ohlcv.timestamp;\n",
    "        '''\n",
    "        )\n",
    "    \n",
    "    postgres_resampled_data = rds_cursor.fetchall()\n",
    "    end_time = time.time()\n",
    "    print(f\"Fetching symbol from Semiconductor industry took in past 5 years took {end_time - start_time:.4f} seconds\")\n",
    "    postgres_resampled_data\n",
    "except Exception as e:\n",
    "    print(f\"Error resampling data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50de132d",
   "metadata": {},
   "source": [
    "## ðŸ¦† DuckDB (S3) Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2afa56c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os   \n",
    "conn = duckdb.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b153b668",
   "metadata": {},
   "source": [
    "### Setup: DuckDB S3 Connection + Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac84df66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data summary:\n",
      "Total rows: 11530\n",
      "Time range: 1980-03-17 05:00:00 to 2025-12-08 05:00:00\n",
      "Unique symbols: 1\n"
     ]
    }
   ],
   "source": [
    "# Test resampling logic with your migrated data using read_parquet\n",
    "# S3 Configuration\n",
    "S3_BUCKET = 'dev-condvest-datalake'\n",
    "S3_PREFIX = 'bronze/raw_ohlcv'\n",
    "AWS_REGION = 'ca-west-1'\n",
    "\n",
    "try:\n",
    "    conn.execute(\"SET s3_region='ca-west-1'\")\n",
    "    conn.execute(f\"SET s3_access_key_id='{os.environ.get('AWS_ACCESS_KEY_ID')}'\")\n",
    "    conn.execute(f\"SET s3_secret_access_key='{os.environ.get('AWS_SECRET_ACCESS_KEY')}'\")\n",
    "\n",
    "    # Path\n",
    "    s3_path = f\"s3://{S3_BUCKET}/{S3_PREFIX}/symbol=AMD/data.parquet\"\n",
    "\n",
    "    # Now create the view as before\n",
    "    conn.execute(f\"\"\"\n",
    "        CREATE OR REPLACE VIEW s3_ohlcv AS\n",
    "        SELECT * FROM read_parquet('{s3_path}');\n",
    "    \"\"\")\n",
    "    \n",
    "    # Check the data structure first\n",
    "    result = conn.execute(\"\"\"\n",
    "        SELECT COUNT(*) as total_rows, \n",
    "            MIN(timestamp) as min_time, \n",
    "            MAX(timestamp) as max_time,\n",
    "            COUNT(DISTINCT symbol) as unique_symbols\n",
    "        FROM s3_ohlcv;\n",
    "    \"\"\").fetchall()\n",
    "    \n",
    "    print(\"Data summary:\")\n",
    "    for row in result:\n",
    "        print(f\"Total rows: {row[0]}\")\n",
    "        print(f\"Time range: {row[1]} to {row[2]}\")\n",
    "        print(f\"Unique symbols: {row[3]}\")\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in resampling: {e}\\n\")\n",
    "    print(\"Tip: Make sure you have set the correct AWS S3 region using DuckDB's SET s3_region statement.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c7f70",
   "metadata": {},
   "source": [
    "### Test 2a: DuckDB Resampling SQL Query\n",
    "\n",
    "**Fibonacci 3-day resampling logic** (same as PostgreSQL test for fair comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26746233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling with DuckDB for selected symbols, using s3_ohlcv view from S3 parquet data\n",
    "sql = \"\"\"\n",
    "    WITH numbered AS (\n",
    "        SELECT\n",
    "            symbol,\n",
    "            CAST(timestamp AS DATE) AS date,\n",
    "            open,\n",
    "            high,\n",
    "            low,\n",
    "            close,\n",
    "            volume,\n",
    "            ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY CAST(timestamp AS DATE)) AS rn\n",
    "        FROM s3_ohlcv\n",
    "        WHERE interval = '1d'\n",
    "        ORDER BY symbol, date\n",
    "    ),\n",
    "    grp AS (\n",
    "        SELECT\n",
    "            symbol,\n",
    "            date,\n",
    "            open,\n",
    "            high,\n",
    "            low,\n",
    "            close,\n",
    "            volume,\n",
    "            CAST(FLOOR((rn - 1) / 3) AS INTEGER) AS grp_id\n",
    "        FROM numbered\n",
    "    ),\n",
    "    aggregated AS (\n",
    "        SELECT\n",
    "            symbol,\n",
    "            grp_id,\n",
    "            MIN(date) AS start_date,\n",
    "            MAX(high) AS high,\n",
    "            MIN(low) AS low,\n",
    "            SUM(volume) AS volume,\n",
    "            FIRST(open ORDER BY date) AS open,\n",
    "            FIRST(close ORDER BY date DESC) AS close\n",
    "        FROM grp\n",
    "        GROUP BY symbol, grp_id\n",
    "        HAVING COUNT(*) = 3\n",
    "    )\n",
    "    SELECT\n",
    "        symbol,\n",
    "        start_date AS date,\n",
    "        open,\n",
    "        high,\n",
    "        low,\n",
    "        close,\n",
    "        volume\n",
    "    FROM aggregated\n",
    "    ORDER BY symbol, date\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3432f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "\n",
      "ðŸ“Š DuckDB with S3 Data:\n",
      "âœ… DuckDB resampling took: 0.3352 seconds\n",
      "âœ… DuckDB processed 3843 rows\n",
      "âœ… DuckDB data shape: (3843, 7)\n",
      "\n",
      "ðŸ“Š PostgreSQL (from earlier test):\n",
      "âœ… PostgreSQL resampling took: 1.2799 seconds\n",
      "âœ… PostgreSQL processed similar data\n",
      "\n",
      "ðŸ† Performance Improvement:\n",
      "âœ… DuckDB is 3.8x faster than PostgreSQL!\n",
      "âœ… Time saved: 0.9446 seconds\n"
     ]
    }
   ],
   "source": [
    "# Performance Comparison: DuckDB vs PostgreSQL\n",
    "import time\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test DuckDB performance with S3 data\n",
    "print(\"\\nðŸ“Š DuckDB with S3 Data:\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Test resampling performance\n",
    "    duckdb_result = conn.execute(sql).fetchdf()\n",
    "    \n",
    "    duckdb_time = time.time() - start_time\n",
    "    print(f\"âœ… DuckDB resampling took: {duckdb_time:.4f} seconds\")\n",
    "    print(f\"âœ… DuckDB processed {len(duckdb_result)} rows\")\n",
    "    print(f\"âœ… DuckDB data shape: {duckdb_result.shape}\")\n",
    "    \n",
    "    # Compare with PostgreSQL (from earlier cells)\n",
    "    print(f\"\\nðŸ“Š PostgreSQL (from earlier test):\")\n",
    "    print(f\"âœ… PostgreSQL resampling took: {postgres_resampling_time:.4f} seconds\")\n",
    "    print(f\"âœ… PostgreSQL processed similar data\")\n",
    "\n",
    "    print(f\"\\nðŸ† Performance Improvement:\")\n",
    "    if 'duckdb_time' in locals():\n",
    "        improvement = postgres_resampling_time / duckdb_time\n",
    "        print(f\"âœ… DuckDB is {improvement:.1f}x faster than PostgreSQL!\")\n",
    "        print(f\"âœ… Time saved: {postgres_resampling_time - duckdb_time:.4f} seconds\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ DuckDB error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9f2330",
   "metadata": {},
   "source": [
    "### ðŸ“ˆ Visualization: Resampled OHLCV Candlestick Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cc9180e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "close": {
          "bdata": "exSuR+FSYUDNzMzMzERgQFyPwvUorF9AuB6F61G4XUDD9Shcj5JfQFyPwvUonF5ASOF6FK5XX0D2KFyPwnVeQPYoXI/CBV1APQrXo3BdXkDD9Shcj8JeQHsUrkfhilxAzczMzMz8XEBxPQrXowBcQB+F61G4nltApHA9CtfzW0Bcj8L1KKxcQNejcD0KB1tAcT0K16PgWEAAAAAAADBZQKRwPQrXE1lAw/UoXI8yWUD2KFyPwiVaQClcj8L1yFpApHA9CtezXECuR+F6FM5ZQD0K16NwvVlAKVyPwvXoVEDNzMzMzCxWQMP1KFyP0ldApHA9CtdjVUCuR+F6FJ5XQKRwPQrXA1hAMzMzMzOzWEDXo3A9ChdZQEjhehSuB1tAj8L1KFy/XEBxPQrXo2BcQKRwPQrXk1tAUrgehetBXECkcD0K11NdQFyPwvUoDF1AKVyPwvVIXkApXI/C9ZhfQEjhehSuB2BAzczMzMzsYUDNzMzMzLxhQIXrUbgePWFAhetRuB5NYUBI4XoUrkdiQIXrUbgeDWRA16NwPQpXY0DXo3A9Cs9kQLgehetRcGZAKVyPwvUYZkDNzMzMzIxlQGZmZmZm3mVAuB6F61EwZkBmZmZmZqZkQOxRuB6Fa2RAw/UoXI8SZUBcj8L1KERkQIXrUbge7WJAPQrXo3B1Y0AfhetRuA5kQBSuR+F6rGNAXI/C9SgcZEDsUbgehStkQI/C9ShcN2VAuB6F61FwakDNzMzMzNxqQDMzMzMz021AUrgehesRbkBI4XoUrl9tQFyPwvUoIHBAUrgehesBcEDhehSuRwVwQOF6FK5HMW1Aj8L1KFx/bkAfhetRuP5uQOF6FK5HyWxAw/UoXI+aaUAzMzMzM8NqQJqZmZmZ2WpAH4XrUbgma0A=",
          "dtype": "f8"
         },
         "high": {
          "bdata": "pHA9CtcDYkBxPQrXo/hgQArXo3A9imBAAAAAAADwX0BxPQrXo+BfQDMzMzMz019ApHA9CtdjX0AfhetRuHZgQD0K16NwrV1Aj8L1KFyvXkCamZmZmWlfQAAAAAAAUF9AXI/C9SgcXkBmZmZmZvZdQKRwPQrXI1xAZmZmZmamXEDXo3A9CgddQDMzMzMzI11AKVyPwvUoW0DsUbgehctZQPYoXI/CdVlArkfhehRuWUCamZmZmYlaQHE9CtejEFtAzczMzMy8XECamZmZmflcQFyPwvUoDFpAzczMzMzcWEAK16NwPXpYQEjhehSuV1hArkfhehSeVkAAAAAAALBXQI/C9ShcX1hAZmZmZmYGWUB7FK5H4dpZQHsUrkfhqltA4XoUrkehXkCPwvUoXI9dQGZmZmZmBl1Aw/UoXI/iXEDsUbgehWtdQDMzMzMz011ArkfhehQeX0AUrkfhegRgQFK4HoXrmWBA9ihcj8IFYkAAAAAAAHhiQI/C9Shch2FAzczMzMyUYUDD9Shcj3JiQB+F61G4PmRAw/UoXI8aZED2KFyPwuVkQFK4HoXryWZAAAAAAADQZkBI4XoUrj9mQArXo3A9WmZAzczMzMxUZ0CamZmZmVlmQClcj8L1EGVASOF6FK5fZUAK16NwPRJlQJqZmZmZQWRAKVyPwvWQZECamZmZmUlkQFyPwvUoNGRAMzMzMzOjZECamZmZmYlkQFK4HoXrYWVAH4XrUbhWbEAzMzMzMwNuQEjhehSu521AXI/C9ShcbkC4HoXrUUhuQOF6FK5HiXBA4XoUrkexcEDD9Shcj05wQLgehetRsG9AzczMzMwcb0Bcj8L1KHhwQK5H4XoUrm9A0ETY8PRobUAfhetRuA5rQI/C9ShcP2xABoGVQ4v0a0A=",
          "dtype": "f8"
         },
         "low": {
          "bdata": "zczMzMwsYUCuR+F6FI5fQEjhehSux15AzczMzMycXUCamZmZmXldQHE9CtejkF5AXI/C9SjcXUBI4XoUrgdeQArXo3A9mlxAPQrXo3BdXUC4HoXrUSheQDMzMzMzM1xAPQrXo3C9XEAAAAAAAKBaQMP1KFyPslpAXI/C9Sg8W0DNzMzMzOxbQB+F61G4/lpAAAAAAADgWEApXI/C9fhXQClcj8L1GFhAH4XrUbiuV0DhehSuR1FYQD0K16NwfVlAhetRuB4VWkAfhetRuJ5ZQDMzMzMzs1hAAAAAAABwU0AfhetRuB5TQAAAAAAAoFZAAAAAAADwVEB7FK5H4UpVQGZmZmZmNldASOF6FK73VkC4HoXrUThYQKRwPQrXY1lAAAAAAABAW0CF61G4HgVcQHsUrkfh6lpAuB6F61EIXEBI4XoUridbQD0K16NwrVxAw/UoXI/CXUCkcD0K18NcQOF6FK5HcV9AFK5H4Xq0X0C4HoXrUaBhQKRwPQrX42BAAAAAAACwYEAzMzMzM7NhQDMzMzMzG2NAexSuR+GqYkAAAAAAAIBjQD0K16NwFWVACtejcD3aZECamZmZmbljQAAAAAAAEGVAAAAAAAAIZkAAAAAAAMhjQNejcD0KN2RAzczMzMyUZECkcD0K15NjQNejcD0Kp2JA9ihcj8L9YkA9CtejcF1jQDMzMzMzu2JAPQrXo3CtY0ApXI/C9VhjQMP1KFyP6mNAFK5H4XpkZEBmZmZmZlZqQM3MzMzM3GpAhetRuB59bEBcj8L1KBxsQGZmZmZmPm5AUrgeheuJb0BSuB6F61luQG3F/rJ7FGxAFK5H4XoUbEAUrkfhelRtQB+F61G4FmxAAAAAAABgaEDNzMzMzEhoQAAAAAAAsGpAlPYGX5h4akA=",
          "dtype": "f8"
         },
         "name": "Candlestick",
         "open": {
          "bdata": "H4XrUbjeYUAfhetRuPZgQAAAAAAAQGBAzczMzMwcX0AAAAAAAIBdQOF6FK5HYV9AZmZmZmbGXkAK16NwPSJgQOxRuB6Fi11AMzMzMzNjXUApXI/C9ZheQDMzMzMzI19A4XoUrkfxXEC4HoXrUXhcQOxRuB6Fu1tAH4XrUbg+W0CamZmZmSlcQHE9CtejwFxAKVyPwvX4WkCkcD0K1+NYQNejcD0KF1lAFK5H4XqkWECuR+F6FO5YQGZmZmZm9llAhetRuB5VWkDXo3A9CodcQI/C9ShcD1lAw/UoXI9yWECamZmZmYlVQHE9CtejoFZA16NwPQrnVUBI4XoUrodVQFK4HoXrUVdApHA9CtczV0AAAAAAAMBYQHsUrkfhellAZmZmZmZmW0AUrkfheoRdQGZmZmZmdlxA7FG4HoVLXECF61G4HuVbQArXo3A9Wl1A7FG4HoXLXUBI4XoUrideQOxRuB6F+19A16NwPQpPYEDXo3A9Cj9iQHE9CtejWGFAUrgehesRYUAAAAAAAOBhQAAAAAAAOGNAexSuR+HyY0AK16NwPYpjQMP1KFyPImVAcT0K16PAZkAK16NwPTJmQOF6FK5HwWVAhetRuB59ZkC4HoXrURhmQOxRuB6Fu2RAzczMzMwUZUBSuB6F69lkQK5H4XoU/mNASOF6FK7/YkAAAAAAAKBjQClcj8L16GNAPQrXo3CtY0AUrkfheqRjQOF6FK5HGWRA9ihcj8JVZUBmZmZmZp5qQGZmZmZmhmtA4XoUrkeJbUAUrkfheuxtQOxRuB6Fa25A16NwPQqDcEDhehSuRzVwQHsUrkfhrm9ArkfhehTebEApXI/C9TRuQOxRuB6FA25AuB6F61HIbECF61G4HuVpQLgehetRBGtAuB6F61EIa0A=",
          "dtype": "f8"
         },
         "type": "candlestick",
         "x": [
          "2024-12-04T00:00:00",
          "2024-12-09T00:00:00",
          "2024-12-12T00:00:00",
          "2024-12-17T00:00:00",
          "2024-12-20T00:00:00",
          "2024-12-26T00:00:00",
          "2024-12-31T00:00:00",
          "2025-01-06T00:00:00",
          "2025-01-10T00:00:00",
          "2025-01-15T00:00:00",
          "2025-01-21T00:00:00",
          "2025-01-24T00:00:00",
          "2025-01-29T00:00:00",
          "2025-02-03T00:00:00",
          "2025-02-06T00:00:00",
          "2025-02-11T00:00:00",
          "2025-02-14T00:00:00",
          "2025-02-20T00:00:00",
          "2025-02-25T00:00:00",
          "2025-02-28T00:00:00",
          "2025-03-05T00:00:00",
          "2025-03-10T00:00:00",
          "2025-03-13T00:00:00",
          "2025-03-18T00:00:00",
          "2025-03-21T00:00:00",
          "2025-03-26T00:00:00",
          "2025-03-31T00:00:00",
          "2025-04-03T00:00:00",
          "2025-04-08T00:00:00",
          "2025-04-11T00:00:00",
          "2025-04-16T00:00:00",
          "2025-04-22T00:00:00",
          "2025-04-25T00:00:00",
          "2025-04-30T00:00:00",
          "2025-05-05T00:00:00",
          "2025-05-08T00:00:00",
          "2025-05-13T00:00:00",
          "2025-05-16T00:00:00",
          "2025-05-21T00:00:00",
          "2025-05-27T00:00:00",
          "2025-05-30T00:00:00",
          "2025-06-04T00:00:00",
          "2025-06-09T00:00:00",
          "2025-06-12T00:00:00",
          "2025-06-17T00:00:00",
          "2025-06-23T00:00:00",
          "2025-06-26T00:00:00",
          "2025-07-01T00:00:00",
          "2025-07-07T00:00:00",
          "2025-07-10T00:00:00",
          "2025-07-15T00:00:00",
          "2025-07-18T00:00:00",
          "2025-07-23T00:00:00",
          "2025-07-28T00:00:00",
          "2025-07-31T00:00:00",
          "2025-08-05T00:00:00",
          "2025-08-08T00:00:00",
          "2025-08-13T00:00:00",
          "2025-08-18T00:00:00",
          "2025-08-21T00:00:00",
          "2025-08-26T00:00:00",
          "2025-08-29T00:00:00",
          "2025-09-04T00:00:00",
          "2025-09-09T00:00:00",
          "2025-09-12T00:00:00",
          "2025-09-17T00:00:00",
          "2025-09-22T00:00:00",
          "2025-09-25T00:00:00",
          "2025-09-30T00:00:00",
          "2025-10-03T00:00:00",
          "2025-10-08T00:00:00",
          "2025-10-13T00:00:00",
          "2025-10-16T00:00:00",
          "2025-10-21T00:00:00",
          "2025-10-24T00:00:00",
          "2025-10-29T00:00:00",
          "2025-11-03T00:00:00",
          "2025-11-06T00:00:00",
          "2025-11-07T00:00:00",
          "2025-11-11T00:00:00",
          "2025-11-14T00:00:00",
          "2025-11-19T00:00:00",
          "2025-11-24T00:00:00",
          "2025-11-28T00:00:00",
          "2025-12-03T00:00:00"
         ],
         "xaxis": "x",
         "yaxis": "y"
        },
        {
         "marker": {
          "color": "lightblue"
         },
         "name": "Volume",
         "type": "bar",
         "x": [
          "2024-12-04T00:00:00",
          "2024-12-09T00:00:00",
          "2024-12-12T00:00:00",
          "2024-12-17T00:00:00",
          "2024-12-20T00:00:00",
          "2024-12-26T00:00:00",
          "2024-12-31T00:00:00",
          "2025-01-06T00:00:00",
          "2025-01-10T00:00:00",
          "2025-01-15T00:00:00",
          "2025-01-21T00:00:00",
          "2025-01-24T00:00:00",
          "2025-01-29T00:00:00",
          "2025-02-03T00:00:00",
          "2025-02-06T00:00:00",
          "2025-02-11T00:00:00",
          "2025-02-14T00:00:00",
          "2025-02-20T00:00:00",
          "2025-02-25T00:00:00",
          "2025-02-28T00:00:00",
          "2025-03-05T00:00:00",
          "2025-03-10T00:00:00",
          "2025-03-13T00:00:00",
          "2025-03-18T00:00:00",
          "2025-03-21T00:00:00",
          "2025-03-26T00:00:00",
          "2025-03-31T00:00:00",
          "2025-04-03T00:00:00",
          "2025-04-08T00:00:00",
          "2025-04-11T00:00:00",
          "2025-04-16T00:00:00",
          "2025-04-22T00:00:00",
          "2025-04-25T00:00:00",
          "2025-04-30T00:00:00",
          "2025-05-05T00:00:00",
          "2025-05-08T00:00:00",
          "2025-05-13T00:00:00",
          "2025-05-16T00:00:00",
          "2025-05-21T00:00:00",
          "2025-05-27T00:00:00",
          "2025-05-30T00:00:00",
          "2025-06-04T00:00:00",
          "2025-06-09T00:00:00",
          "2025-06-12T00:00:00",
          "2025-06-17T00:00:00",
          "2025-06-23T00:00:00",
          "2025-06-26T00:00:00",
          "2025-07-01T00:00:00",
          "2025-07-07T00:00:00",
          "2025-07-10T00:00:00",
          "2025-07-15T00:00:00",
          "2025-07-18T00:00:00",
          "2025-07-23T00:00:00",
          "2025-07-28T00:00:00",
          "2025-07-31T00:00:00",
          "2025-08-05T00:00:00",
          "2025-08-08T00:00:00",
          "2025-08-13T00:00:00",
          "2025-08-18T00:00:00",
          "2025-08-21T00:00:00",
          "2025-08-26T00:00:00",
          "2025-08-29T00:00:00",
          "2025-09-04T00:00:00",
          "2025-09-09T00:00:00",
          "2025-09-12T00:00:00",
          "2025-09-17T00:00:00",
          "2025-09-22T00:00:00",
          "2025-09-25T00:00:00",
          "2025-09-30T00:00:00",
          "2025-10-03T00:00:00",
          "2025-10-08T00:00:00",
          "2025-10-13T00:00:00",
          "2025-10-16T00:00:00",
          "2025-10-21T00:00:00",
          "2025-10-24T00:00:00",
          "2025-10-29T00:00:00",
          "2025-11-03T00:00:00",
          "2025-11-06T00:00:00",
          "2025-11-07T00:00:00",
          "2025-11-11T00:00:00",
          "2025-11-14T00:00:00",
          "2025-11-19T00:00:00",
          "2025-11-24T00:00:00",
          "2025-11-28T00:00:00",
          "2025-12-03T00:00:00"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AAAAQAM+lUEAAABoyt+gQQAAACiLJ6FBAAAAAAyYoEEAAACwjhaeQQAAAOD1FJVBAAAAkH8jmEEAAADQfvCfQQAAANCmKKBBAAAAoPKkmEEAAADQKdKVQQAAAKB0wZ9BAAAAUGNVmkEAAABQdo6pQQAAAJDnVJ9BAAAAUL4kmEEAAABAliyWQQAAAPDOR5dBAAAAcFeQm0EAAAD43nygQQAAAND0epVBAAAAAPyFmUEAAACAMB+XQQAAAJASlJRBAAAAIGn8mkEAAADAUbeYQQAAAJDG7JFBAAAAqL9FpkEAAADQA8WrQQAAABAHF51BAAAAoE8tn0EAAABAKTuaQQAAAOB9ApNBAAAAkDlJlUEAAABgbrKlQQAAAPgCRaBBAAAAAMozpkEAAADwVUWaQQAAALAxuphBAAAAUHLblkEAAABAzpaZQQAAAFDi+JVBAAAAMG7ZnkEAAABApBmmQQAAAOBZxalBAAAAiPcGqkEAAACIIHGjQQAAAABVW51BAAAAcGFamkEAAADowJSiQQAAAMBZQahBAAAAyCBSoEEAAAAIthehQQAAACgmwqxBAAAAIPLap0EAAABov/KyQQAAAGAW3KZBAAAAkMP1qkEAAABI6CWjQQAAAKD9IpxBAAAAQCfpnUEAAAAQOn6ZQQAAAOgIJaJBAAAA+CMtoUEAAACAB4KZQQAAALC/rKVBAAAAsN2pnUEAAABADYCZQQAAAFDRz51BAAAAc85FuEEAAAATeTq2QQAAAMi08axBAAAAnoK6pUEAAABg8GGhQQAAABrZ96VBAAAAVFC+nkEAAADQNBmjQQAAAIA196VBAAAAaHKOoEEAAADgg9SrQQAAABD0Rp9BAAAARHfHo0EAAACAoHyfQQAAAPDQVZNBAAAA9GvQkUE=",
          "dtype": "f8"
         },
         "yaxis": "y2"
        }
       ],
       "layout": {
        "height": 700,
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Candlestick and Volume Chart for Last 365 Days"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "matches": "x2",
         "rangeslider": {
          "visible": false
         },
         "showticklabels": false,
         "title": {
          "text": "Date"
         }
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.314,
          1
         ],
         "title": {
          "text": "Price"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          0.294
         ],
         "title": {
          "text": "Volume"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the resampled data for last 100 days\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Candle plot for last 100 days\n",
    "filtered_data = duckdb_result[(duckdb_result['symbol'] == 'AMD') & (duckdb_result['date'] >= (duckdb_result['date'].max() - pd.Timedelta(days=365)))]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.02,\n",
    "    row_heights=[0.7, 0.3]  # Candlestick gets more height than volume\n",
    ")\n",
    "\n",
    "# Add candlestick trace to row=1, col=1\n",
    "fig.add_trace(\n",
    "    go.Candlestick(\n",
    "        x=filtered_data['date'],\n",
    "        open=filtered_data['open'],\n",
    "        high=filtered_data['high'],\n",
    "        low=filtered_data['low'],\n",
    "        close=filtered_data['close'],\n",
    "        name='Candlestick'\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "# Add volume trace to row=2, col=1\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=filtered_data['date'],\n",
    "        y=filtered_data['volume'],\n",
    "        name='Volume',\n",
    "        marker_color='lightblue'\n",
    "    ),\n",
    "    row=2,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "# Update y-axes titles individually\n",
    "fig.update_yaxes(title_text='Price', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Volume', row=2, col=1)\n",
    "\n",
    "# General layout updates\n",
    "fig.update_layout(\n",
    "    title='Candlestick and Volume Chart for Last 365 Days',\n",
    "    xaxis=dict(title='Date', rangeslider_visible=False),\n",
    "    height=700,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330c80b",
   "metadata": {},
   "source": [
    "### Test 2b: S3 File Discovery (Latest Timestamp per Symbol)\n",
    "\n",
    "**Goal**: Efficiently discover latest data for each symbol in S3 bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db6d7397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbols with data.parquet:\n",
      "                      latest_dt        source\n",
      "symbol                                       \n",
      "symbol=A    2025-12-08 05:00:00  data.parquet\n",
      "symbol=AACT 2025-09-24 04:00:00  data.parquet\n",
      "symbol=AACB 2025-12-08 05:00:00  data.parquet\n",
      "symbol=AAM  2025-12-08 05:00:00  data.parquet\n",
      "symbol=AACI 2025-10-29 04:00:00  data.parquet\n",
      "...                         ...           ...\n",
      "symbol=ZYBT 2025-12-08 05:00:00  data.parquet\n",
      "symbol=ZVRA 2025-12-08 05:00:00  data.parquet\n",
      "symbol=ZYXI 2025-12-08 05:00:00  data.parquet\n",
      "symbol=ZWS  2025-12-08 05:00:00  data.parquet\n",
      "symbol=ZYME 2025-12-08 05:00:00  data.parquet\n",
      "\n",
      "[5419 rows x 2 columns]\n",
      "No symbols lacking data.parquet found.\n",
      "Total symbols processed: 5419\n"
     ]
    }
   ],
   "source": [
    "# Enhanced version: Handle both data.parquet and date=xxx parquet files, and optimized listing with threading\n",
    "\n",
    "import boto3\n",
    "import pyarrow.parquet as pq\n",
    "import io\n",
    "import concurrent.futures\n",
    "\n",
    "S3_BUCKET = 'dev-condvest-datalake'\n",
    "S3_PREFIX = 'bronze/raw_ohlcv'\n",
    "AWS_REGION = 'ca-west-1'\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "\n",
    "def list_symbols():\n",
    "    \"\"\"List all symbol folders in the S3 prefix.\"\"\"\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    result = paginator.paginate(Bucket=S3_BUCKET, Prefix=S3_PREFIX + '/', Delimiter='/')\n",
    "    symbols = []\n",
    "    for page in result:\n",
    "        if \"CommonPrefixes\" in page:\n",
    "            for prefix in page[\"CommonPrefixes\"]:\n",
    "                folder = prefix[\"Prefix\"]\n",
    "                symbol = folder[len(S3_PREFIX)+1:].rstrip(\"/\")\n",
    "                symbols.append(symbol)\n",
    "    return symbols\n",
    "\n",
    "def get_latest_timestamp_for_symbol(symbol):\n",
    "    \"\"\"Try to read latest timestamp from data.parquet, else check for partitioned parquet files (date=xxx).\"\"\"\n",
    "    parquet_s3_key = f\"{S3_PREFIX}/{symbol}/data.parquet\"\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=S3_BUCKET, Key=parquet_s3_key)\n",
    "        data = response[\"Body\"].read()\n",
    "        table = pq.read_table(io.BytesIO(data), columns=[\"timestamp\"])\n",
    "        df = table.to_pandas()\n",
    "        latest_dt = df['timestamp'].max()\n",
    "        return {\"symbol\": symbol, \"latest_dt\": latest_dt, \"source\": \"data.parquet\"}\n",
    "    except Exception:\n",
    "        # Try to find date=xxx.parquet files\n",
    "        paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
    "        page_it = paginator.paginate(Bucket=S3_BUCKET, Prefix=f\"{S3_PREFIX}/{symbol}/date=\")\n",
    "        max_date = None\n",
    "        file_found = False\n",
    "        for page in page_it:\n",
    "            if \"Contents\" in page:\n",
    "                for obj in page[\"Contents\"]:\n",
    "                    key = obj[\"Key\"]\n",
    "                    if key.endswith(\".parquet\"):\n",
    "                        file_found = True\n",
    "                        try:\n",
    "                            resp = s3_client.get_object(Bucket=S3_BUCKET, Key=key)\n",
    "                            d = resp['Body'].read()\n",
    "                            tab = pq.read_table(io.BytesIO(d), columns=[\"timestamp\"])\n",
    "                            df = tab.to_pandas()\n",
    "                            val = df['timestamp'].max()\n",
    "                            if max_date is None or (val is not None and val > max_date):\n",
    "                                max_date = val\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error reading {key}: {e}\")\n",
    "        if not file_found:\n",
    "            return {\"symbol\": symbol, \"latest_dt\": None, \"source\": \"none\"}\n",
    "        return {\"symbol\": symbol, \"latest_dt\": max_date, \"source\": \"partitioned_parquet\"}\n",
    "\n",
    "# Optimized: process symbols in parallel (increase workers for even faster results, depending on hardware/network)\n",
    "symbols = list_symbols()\n",
    "\n",
    "results_with_data_parquet = []\n",
    "results_without_data_parquet = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=12) as executor:\n",
    "    future_to_symbol = {executor.submit(get_latest_timestamp_for_symbol, sym): sym for sym in symbols}\n",
    "    for future in concurrent.futures.as_completed(future_to_symbol):\n",
    "        info = future.result()\n",
    "        if info[\"source\"] == \"data.parquet\":\n",
    "            results_with_data_parquet.append(info)\n",
    "        elif info[\"source\"] == \"partitioned_parquet\":\n",
    "            results_without_data_parquet.append(info)\n",
    "        else:\n",
    "            results_without_data_parquet.append(info)\n",
    "\n",
    "# Make quick summary tables\n",
    "import pandas as pd\n",
    "\n",
    "if results_with_data_parquet:\n",
    "    df_data_parquet = pd.DataFrame(results_with_data_parquet).set_index(\"symbol\")\n",
    "    print(\"Symbols with data.parquet:\")\n",
    "    print(df_data_parquet)\n",
    "else:\n",
    "    print(\"No symbols with data.parquet found.\")\n",
    "\n",
    "if results_without_data_parquet:\n",
    "    df_non_data_parquet = pd.DataFrame(results_without_data_parquet).set_index(\"symbol\")\n",
    "    print(\"Symbols without data.parquet (but with partitioned date=xxx.parquet, or none):\")\n",
    "    print(df_non_data_parquet)\n",
    "else:\n",
    "    print(\"No symbols lacking data.parquet found.\")\n",
    "\n",
    "print(f\"Total symbols processed: {len(symbols)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba64f1b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“‚ Performance Benchmark 2: DuckDB vs Polars for S3 Parquet Reading\n",
    "\n",
    "**Goal**: Test which library is faster for reading parquet files from S3 before implementing consolidation job\n",
    "\n",
    "**Test Scenarios**:\n",
    "1. Read single symbol's `date=*.parquet` files (current fetcher format)\n",
    "2. Read multiple symbols with wildcard vs explicit paths\n",
    "3. Memory usage comparison\n",
    "\n",
    "**Expected Outcome**: DuckDB httpfs performs well, but wildcards have S3 listing overhead\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666c36a1",
   "metadata": {},
   "source": [
    "## Setup: S3 Configuration & Symbol Discovery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc1cd95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 symbols for testing\n",
      "First 10 symbols: ['A', 'AA', 'AACB', 'AACG', 'AACI', 'AACT', 'AAL', 'AAM', 'AAME', 'AAMI']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import psutil\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "# S3 Configuration\n",
    "S3_BUCKET = 'dev-condvest-datalake'\n",
    "S3_PREFIX = 'bronze/raw_ohlcv'\n",
    "AWS_REGION = 'ca-west-1'\n",
    "\n",
    "# Get list of symbols to test (first 100 symbols from S3)\n",
    "s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "\n",
    "def list_symbols(limit=100):\n",
    "    \"\"\"List symbols from S3\"\"\"\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    symbols = set()\n",
    "    \n",
    "    for page in paginator.paginate(\n",
    "        Bucket=S3_BUCKET,\n",
    "        Prefix=f\"{S3_PREFIX}/symbol=\",\n",
    "        Delimiter='/'\n",
    "    ):\n",
    "        if 'CommonPrefixes' in page:\n",
    "            for prefix in page['CommonPrefixes']:\n",
    "                symbol = prefix['Prefix'].split('symbol=')[-1].rstrip('/')\n",
    "                symbols.add(symbol)\n",
    "                if len(symbols) >= limit:\n",
    "                    break\n",
    "        if len(symbols) >= limit:\n",
    "            break\n",
    "    \n",
    "    return sorted(list(symbols))\n",
    "\n",
    "# Get test symbols\n",
    "test_symbols = list_symbols(limit=100)\n",
    "print(f\"Found {len(test_symbols)} symbols for testing\")\n",
    "print(f\"First 10 symbols: {test_symbols[:10]}\")\n",
    "\n",
    "# Memory tracking helper\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / 1024 / 1024  # Convert to MB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08cdbdf",
   "metadata": {},
   "source": [
    "## Test 3a: DuckDB vs Polars - Single Symbol Read\n",
    "\n",
    "Reading date-partitioned files: `symbol=AAPL/date=2025-11-23.parquet`\n",
    "(Format currently written by Lambda fetcher)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ebbd81a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing with symbol: A\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š DuckDB - Reading date-partitioned files\n",
      "âœ… Rows read: 21\n",
      "âœ… Time: 0.466s\n",
      "âœ… Memory used: 21.6 MB\n",
      "\n",
      "ðŸ“Š Polars - Reading date-partitioned files\n",
      "âŒ Polars error: data type mismatch for column timestamp: expected: datetime[ns], found: datetime[Î¼s]\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š COMPARISON - Reading date-partitioned files:\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test reading date-partitioned files: symbol=AAPL/date=2025-11-23.parquet\n",
    "This is what the Lambda fetcher currently writes\n",
    "\"\"\"\n",
    "\n",
    "test_symbol = test_symbols[0] if test_symbols else 'AAPL'\n",
    "print(f\"ðŸ§ª Testing with symbol: {test_symbol}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- DuckDB Test ---\n",
    "print(\"\\nðŸ“Š DuckDB - Reading date-partitioned files\")\n",
    "mem_before_duck = get_memory_usage()\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    conn_duck = duckdb.connect(':memory:')\n",
    "    conn_duck.execute(\"INSTALL httpfs;\")\n",
    "    conn_duck.execute(\"LOAD httpfs;\")\n",
    "    conn_duck.execute(f\"SET s3_region='{AWS_REGION}';\")\n",
    "    \n",
    "    # Read all date=*.parquet files for this symbol\n",
    "    s3_path = f\"s3://{S3_BUCKET}/{S3_PREFIX}/symbol={test_symbol}/date=*.parquet\"\n",
    "    \n",
    "    df_duck = conn_duck.execute(f\"\"\"\n",
    "        SELECT * FROM read_parquet('{s3_path}')\n",
    "    \"\"\").fetchdf()\n",
    "    \n",
    "    duck_time = time.time() - start_time\n",
    "    mem_after_duck = get_memory_usage()\n",
    "    \n",
    "    print(f\"âœ… Rows read: {len(df_duck):,}\")\n",
    "    print(f\"âœ… Time: {duck_time:.3f}s\")\n",
    "    print(f\"âœ… Memory used: {mem_after_duck - mem_before_duck:.1f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ DuckDB error: {e}\")\n",
    "    duck_time = None\n",
    "\n",
    "# --- Polars Test ---\n",
    "print(\"\\nðŸ“Š Polars - Reading date-partitioned files\")\n",
    "mem_before_polars = get_memory_usage()\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Configure Polars for S3\n",
    "    storage_options = {\n",
    "        'aws_region': AWS_REGION,\n",
    "    }\n",
    "    \n",
    "    # Polars can read multiple parquet files with glob pattern\n",
    "    s3_path = f\"s3://{S3_BUCKET}/{S3_PREFIX}/symbol={test_symbol}/date=*.parquet\"\n",
    "    \n",
    "    df_polars = pl.read_parquet(s3_path, storage_options=storage_options)\n",
    "    \n",
    "    polars_time = time.time() - start_time\n",
    "    mem_after_polars = get_memory_usage()\n",
    "    \n",
    "    print(f\"âœ… Rows read: {len(df_polars):,}\")\n",
    "    print(f\"âœ… Time: {polars_time:.3f}s\")\n",
    "    print(f\"âœ… Memory used: {mem_after_polars - mem_before_polars:.1f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Polars error: {e}\")\n",
    "    polars_time = None\n",
    "\n",
    "# --- Comparison ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“Š COMPARISON - Reading date-partitioned files:\")\n",
    "if duck_time and polars_time:\n",
    "    if polars_time < duck_time:\n",
    "        speedup = duck_time / polars_time\n",
    "        print(f\"ðŸ† Polars is {speedup:.2f}x faster than DuckDB!\")\n",
    "    else:\n",
    "        speedup = polars_time / duck_time\n",
    "        print(f\"ðŸ† DuckDB is {speedup:.2f}x faster than Polars!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde28d08",
   "metadata": {},
   "source": [
    "## âš™ï¸ AWS Credentials Check\n",
    "\n",
    "**Verify credentials before running S3 tests:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd447bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check AWS credentials\n",
    "print(\"ðŸ” AWS Credentials Status:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "aws_key = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "aws_secret = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "if aws_key and aws_secret:\n",
    "    print(\"âœ… AWS_ACCESS_KEY_ID found:\", aws_key[:8] + \"...\" + aws_key[-4:])\n",
    "    print(\"âœ… AWS_SECRET_ACCESS_KEY found: ****\" + aws_secret[-4:])\n",
    "    print(\"\\nâœ… Credentials configured! You can run the S3 tests below.\")\n",
    "else:\n",
    "    print(\"âŒ AWS credentials NOT found in environment variables!\")\n",
    "    print(\"\\nðŸ’¡ To fix this, run:\")\n",
    "    print(\"   export AWS_ACCESS_KEY_ID='your_key_here'\")\n",
    "    print(\"   export AWS_SECRET_ACCESS_KEY='your_secret_here'\")\n",
    "    print(\"\\n   Or add them to your .env file and restart Jupyter\")\n",
    "    \n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1283f3",
   "metadata": {},
   "source": [
    "## Test 3b: Wildcard vs Explicit Paths (S3 Performance Issue)\n",
    "\n",
    "**Problem Discovered:**\n",
    "- 1 symbol = instant  \n",
    "- 2+ symbols with wildcard = VERY SLOW\n",
    "\n",
    "**Root cause:** `symbol=*/date=*.parquet` scans ALL 5,350 symbols before filtering!\n",
    "\n",
    "**Solution:** Use explicit paths `['symbol=AAPL/date=*.parquet', 'symbol=MSFT/date=*.parquet']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8f97a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Demonstrate the S3 listing performance problem and solution\n",
    "\"\"\"\n",
    "\n",
    "# Test with 2 symbols\n",
    "test_symbols_2 = test_symbols[:2] if len(test_symbols) >= 2 else ['AAPL', 'MSFT']\n",
    "print(f\"ðŸ§ª Testing with 2 symbols: {test_symbols_2}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- APPROACH 1: Wildcard with Filter (SLOW) âŒ ---\n",
    "print(\"\\nâŒ APPROACH 1: Wildcard pattern with WHERE filter\")\n",
    "print(\"   Pattern: symbol=*/date=*.parquet WHERE symbol IN (...)\")\n",
    "print(\"   Problem: S3 lists ALL 5,350 symbol directories first!\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    conn_test = duckdb.connect(':memory:')\n",
    "    conn_test.execute(\"INSTALL httpfs;\")\n",
    "    conn_test.execute(\"LOAD httpfs;\")\n",
    "    conn_test.execute(f\"SET s3_region='{AWS_REGION}';\")\n",
    "    \n",
    "    # Configure AWS credentials (from environment)\n",
    "    if os.environ.get('AWS_ACCESS_KEY_ID'):\n",
    "        conn_test.execute(f\"SET s3_access_key_id='{os.environ.get('AWS_ACCESS_KEY_ID')}';\")\n",
    "        conn_test.execute(f\"SET s3_secret_access_key='{os.environ.get('AWS_SECRET_ACCESS_KEY')}';\")\n",
    "        print(\"   âœ… AWS credentials configured\")\n",
    "    else:\n",
    "        print(\"   âš ï¸  No AWS credentials found in environment\")\n",
    "    \n",
    "    # This will be SLOW because it scans all symbols\n",
    "    s3_path = f\"s3://{S3_BUCKET}/{S3_PREFIX}/symbol=*/date=*.parquet\"\n",
    "    symbols_filter = \"', '\".join(test_symbols_2)\n",
    "    \n",
    "    print(f\"   Executing query... (this will take a while)\")\n",
    "    df_wildcard = conn_test.execute(f\"\"\"\n",
    "        SELECT COUNT(*) as row_count, COUNT(DISTINCT symbol) as symbol_count\n",
    "        FROM read_parquet('{s3_path}')\n",
    "        WHERE symbol IN ('{symbols_filter}')\n",
    "    \"\"\").fetchdf()\n",
    "    \n",
    "    wildcard_time = time.time() - start_time\n",
    "    print(f\"   â±ï¸  Time: {wildcard_time:.2f}s\")\n",
    "    print(f\"   ðŸ“Š Result: {df_wildcard.values[0]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Error (or timeout): {str(e)[:100]}...\")\n",
    "    wildcard_time = None\n",
    "\n",
    "# --- APPROACH 2: Explicit Paths (FAST) âœ… ---\n",
    "print(\"\\nâœ… APPROACH 2: Explicit symbol paths (NO wildcard)\")\n",
    "print(\"   Pattern: ['symbol=AAPL/date=*.parquet', 'symbol=MSFT/date=*.parquet']\")\n",
    "print(\"   Benefit: Only lists the 2 specific directories!\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    conn_test2 = duckdb.connect(':memory:')\n",
    "    conn_test2.execute(\"INSTALL httpfs;\")\n",
    "    conn_test2.execute(\"LOAD httpfs;\")\n",
    "    conn_test2.execute(f\"SET s3_region='{AWS_REGION}';\")\n",
    "    \n",
    "    # Configure AWS credentials (from environment)\n",
    "    if os.environ.get('AWS_ACCESS_KEY_ID'):\n",
    "        conn_test2.execute(f\"SET s3_access_key_id='{os.environ.get('AWS_ACCESS_KEY_ID')}';\")\n",
    "        conn_test2.execute(f\"SET s3_secret_access_key='{os.environ.get('AWS_SECRET_ACCESS_KEY')}';\")\n",
    "        print(\"   âœ… AWS credentials configured\")\n",
    "    else:\n",
    "        print(\"   âš ï¸  No AWS credentials found in environment\")\n",
    "    \n",
    "    # Build explicit paths\n",
    "    explicit_paths = [\n",
    "        f\"s3://{S3_BUCKET}/{S3_PREFIX}/symbol={symbol}/date=*.parquet\"\n",
    "        for symbol in test_symbols_2\n",
    "    ]\n",
    "    \n",
    "    # Use array of paths\n",
    "    paths_str = \"', '\".join(explicit_paths)\n",
    "    \n",
    "    print(f\"   Executing query...\")\n",
    "    df_explicit = conn_test2.execute(f\"\"\"\n",
    "        SELECT COUNT(*) as row_count, COUNT(DISTINCT symbol) as symbol_count\n",
    "        FROM read_parquet(['{paths_str}'])\n",
    "    \"\"\").fetchdf()\n",
    "    \n",
    "    explicit_time = time.time() - start_time\n",
    "    print(f\"   â±ï¸  Time: {explicit_time:.2f}s\")\n",
    "    print(f\"   ðŸ“Š Result: {df_explicit.values[0]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Error: {str(e)[:100]}...\")\n",
    "    explicit_time = None\n",
    "\n",
    "# --- COMPARISON ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ† PERFORMANCE COMPARISON:\")\n",
    "if wildcard_time and explicit_time:\n",
    "    speedup = wildcard_time / explicit_time\n",
    "    print(f\"   Explicit paths are {speedup:.1f}x FASTER!\")\n",
    "    print(f\"   Time saved: {wildcard_time - explicit_time:.1f}s for just 2 symbols\")\n",
    "    print(f\"\\n   ðŸ“ˆ For 100 symbols:\")\n",
    "    print(f\"      Wildcard approach: ~{wildcard_time:.1f}s (doesn't scale!)\")\n",
    "    print(f\"      Explicit approach: ~{explicit_time * 50:.1f}s (scales linearly)\")\n",
    "elif explicit_time:\n",
    "    print(f\"   âœ… Explicit paths completed in {explicit_time:.2f}s\")\n",
    "    print(f\"   âŒ Wildcard approach timed out or failed\")\n",
    "    \n",
    "print(\"\\nðŸ’¡ KEY INSIGHT:\")\n",
    "print(\"   - Wildcard '*' forces S3 to list ALL symbols before filtering\")\n",
    "print(\"   - Explicit paths only list the directories you need\")\n",
    "print(\"   - This is why your resampler times out!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ee11e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ—œï¸ Performance Benchmark 3: Bronze Layer Consolidation\n",
    "\n",
    "**Goal**: Test consolidation performance for merging `date=*.parquet` â†’ `data.parquet`\n",
    "\n",
    "**Test Scenarios**:\n",
    "1. Single symbol consolidation (incremental merge)\n",
    "2. Batch consolidation (10 symbols sequential)\n",
    "3. Lambda runtime estimation\n",
    "4. Parallel vs Sequential consolidation comparison\n",
    "\n",
    "**Architecture**: Uses same logic as `consolidator.py` AWS Batch job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50bfa0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Consolidation Performance Test Setup\n",
      "============================================================\n",
      "S3 Bucket: dev-condvest-datalake\n",
      "S3 Prefix: bronze/raw_ohlcv\n",
      "Region: ca-west-1\n",
      "Retention Days: 30\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Consolidation Performance Test\n",
    "Using the same logic as daily_consolidate.py Lambda function\n",
    "\n",
    "We'll test symbols that have been vacuumed (A-EBF range)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import duckdb\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from io import BytesIO\n",
    "from datetime import datetime, date, timedelta\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Configuration\n",
    "S3_BUCKET = 'dev-condvest-datalake'\n",
    "S3_PREFIX = 'bronze/raw_ohlcv'\n",
    "AWS_REGION = 'ca-west-1'\n",
    "RETENTION_DAYS = 30\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "\n",
    "print(\"ðŸ”§ Consolidation Performance Test Setup\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"S3 Bucket: {S3_BUCKET}\")\n",
    "print(f\"S3 Prefix: {S3_PREFIX}\")\n",
    "print(f\"Region: {AWS_REGION}\")\n",
    "print(f\"Retention Days: {RETENTION_DAYS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e21cf",
   "metadata": {},
   "source": [
    "## Setup: Helper Functions (Same as consolidator.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198d2ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper functions defined\n",
      "ðŸ“Š Found 200 symbols in vacuumed range (A-EBF)\n",
      "   First 10: ['A', 'AA', 'AACB', 'AACG', 'AACI', 'AACT', 'AAL', 'AAM', 'AAME', 'AAMI']\n",
      "   Last 10: ['AKAN', 'AKBA', 'AKO.A', 'AKO.B', 'AKR', 'AKRO', 'AKTX', 'AL', 'ALAB', 'ALAR']\n"
     ]
    }
   ],
   "source": [
    "def init_duckdb() -> duckdb.DuckDBPyConnection:\n",
    "    \"\"\"Initialize DuckDB with S3 credentials (same as Lambda)\"\"\"\n",
    "    conn = duckdb.connect(':memory:')\n",
    "    conn.execute(\"INSTALL httpfs;\")\n",
    "    conn.execute(\"LOAD httpfs;\")\n",
    "    conn.execute(f\"SET s3_region='{AWS_REGION}'\")\n",
    "    \n",
    "    # Get credentials from environment\n",
    "    aws_access_key = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "    aws_secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "    \n",
    "    if aws_access_key and aws_secret_key:\n",
    "        conn.execute(f\"SET s3_access_key_id='{aws_access_key}'\")\n",
    "        conn.execute(f\"SET s3_secret_access_key='{aws_secret_key}'\")\n",
    "        print(\"âœ… DuckDB initialized with AWS credentials\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No AWS credentials found - using instance profile\")\n",
    "    \n",
    "    return conn\n",
    "\n",
    "def list_date_files(symbol: str, after_date: Optional[date] = None) -> List[Dict]:\n",
    "    \"\"\"List date=*.parquet files for a symbol (same as Lambda)\"\"\"\n",
    "    prefix = f\"{S3_PREFIX}/symbol={symbol}/\"\n",
    "    date_files = []\n",
    "    \n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    \n",
    "    for page in paginator.paginate(Bucket=S3_BUCKET, Prefix=prefix):\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            filename = key.split('/')[-1]\n",
    "            \n",
    "            if filename.startswith('date=') and filename.endswith('.parquet'):\n",
    "                try:\n",
    "                    date_str = filename.replace('date=', '').replace('.parquet', '')\n",
    "                    file_date = datetime.strptime(date_str, '%Y-%m-%d').date()\n",
    "                    \n",
    "                    # Filter by date if specified\n",
    "                    if after_date and file_date <= after_date:\n",
    "                        continue\n",
    "                    \n",
    "                    date_files.append({\n",
    "                        'key': key,\n",
    "                        'date': file_date,\n",
    "                        'size': obj['Size']\n",
    "                    })\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    \n",
    "    return sorted(date_files, key=lambda x: x['date'])\n",
    "\n",
    "def check_data_parquet_exists(symbol: str) -> bool:\n",
    "    \"\"\"Check if data.parquet exists for a symbol (same as Lambda)\"\"\"\n",
    "    key = f\"{S3_PREFIX}/symbol={symbol}/data.parquet\"\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=S3_BUCKET, Key=key)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def list_vacuumed_symbols(limit: int = 100) -> List[str]:\n",
    "    \"\"\"List symbols that have been vacuumed (have data.parquet and minimal date files)\"\"\"\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    symbols = []\n",
    "    \n",
    "    for page in paginator.paginate(\n",
    "        Bucket=S3_BUCKET,\n",
    "        Prefix=f\"{S3_PREFIX}/symbol=\",\n",
    "        Delimiter='/'\n",
    "    ):\n",
    "        for prefix in page.get('CommonPrefixes', []):\n",
    "            symbol = prefix['Prefix'].split('symbol=')[1].rstrip('/')\n",
    "            # Only include symbols A-EBF (alphabetically vacuumed)\n",
    "            if symbol <= 'EBF':\n",
    "                symbols.append(symbol)\n",
    "            if len(symbols) >= limit:\n",
    "                break\n",
    "        if len(symbols) >= limit:\n",
    "            break\n",
    "    \n",
    "    return sorted(symbols)\n",
    "\n",
    "print(\"âœ… Helper functions defined\")\n",
    "vacuumed_symbols = list_vacuumed_symbols(limit=200)\n",
    "print(f\"ðŸ“Š Found {len(vacuumed_symbols)} symbols in vacuumed range (A-EBF)\")\n",
    "print(f\"   First 10: {vacuumed_symbols[:10]}\")\n",
    "print(f\"   Last 10: {vacuumed_symbols[-10:]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192a7300",
   "metadata": {},
   "source": [
    "## Test 4a: Single Symbol Consolidation\n",
    "\n",
    "Tests the core consolidation logic for a single symbol (incremental merge of `data.parquet` + new `date=*.parquet` files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f0c652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ§ª TEST 1: Single Symbol Consolidation\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Testing symbol: A\n",
      "âœ… DuckDB initialized with AWS credentials\n",
      "  ðŸ“ data.parquet exists: True\n",
      "  ðŸ“… date=*.parquet files: 24\n",
      "  ðŸ“¥ Mode: Incremental (data.parquet + 24 date files)\n",
      "  âœ… Read 6,532 rows in 0.70s\n",
      "  â¸ï¸  Dry run - skipped S3 write\n",
      "\n",
      "ðŸ“Š Result:\n",
      "   symbol: A\n",
      "   status: success\n",
      "   rows_consolidated: 6532\n",
      "   files_read: 25\n",
      "   read_time_s: 0.7027058601379395\n",
      "   write_time_s: 0\n",
      "   total_time_s: 0.8356640338897705\n",
      "   has_data_parquet: True\n",
      "   date_files_count: 24\n"
     ]
    }
   ],
   "source": [
    "def consolidate_symbol(\n",
    "    conn: duckdb.DuckDBPyConnection,\n",
    "    symbol: str,\n",
    "    dry_run: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Consolidate a single symbol's date files into data.parquet\n",
    "    Same logic as daily_consolidate.py Lambda function\n",
    "    \n",
    "    Args:\n",
    "        conn: DuckDB connection with S3 configured\n",
    "        symbol: Stock symbol\n",
    "        dry_run: If True, don't write to S3 (just test read performance)\n",
    "    \n",
    "    Returns:\n",
    "        Dict with consolidation results\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'symbol': symbol,\n",
    "        'status': 'success',\n",
    "        'rows_consolidated': 0,\n",
    "        'files_read': 0,\n",
    "        'read_time_s': 0,\n",
    "        'write_time_s': 0,\n",
    "        'total_time_s': 0,\n",
    "        'has_data_parquet': False,\n",
    "        'date_files_count': 0\n",
    "    }\n",
    "    \n",
    "    total_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        s3_symbol_path = f\"s3://{S3_BUCKET}/{S3_PREFIX}/symbol={symbol}\"\n",
    "        data_parquet_path = f\"{s3_symbol_path}/data.parquet\"\n",
    "        \n",
    "        # Check current state\n",
    "        has_existing_data = check_data_parquet_exists(symbol)\n",
    "        date_files = list_date_files(symbol)\n",
    "        \n",
    "        result['has_data_parquet'] = has_existing_data\n",
    "        result['date_files_count'] = len(date_files)\n",
    "        \n",
    "        print(f\"  ðŸ“ data.parquet exists: {has_existing_data}\")\n",
    "        print(f\"  ðŸ“… date=*.parquet files: {len(date_files)}\")\n",
    "        \n",
    "        if not date_files and not has_existing_data:\n",
    "            result['status'] = 'skipped'\n",
    "            result['reason'] = 'No files found'\n",
    "            return result\n",
    "        \n",
    "        # Build SQL query based on what files exist\n",
    "        read_start = time.time()\n",
    "        \n",
    "        if has_existing_data and date_files:\n",
    "            # BOTH exist: Union data.parquet + date files (incremental consolidation)\n",
    "            date_paths = [f\"s3://{S3_BUCKET}/{f['key']}\" for f in date_files]\n",
    "            paths_str = \"', '\".join(date_paths)\n",
    "            \n",
    "            merge_sql = f\"\"\"\n",
    "                SELECT * FROM read_parquet('{data_parquet_path}')\n",
    "                UNION ALL\n",
    "                SELECT * FROM read_parquet(['{paths_str}'])\n",
    "            \"\"\"\n",
    "            result['files_read'] = 1 + len(date_files)\n",
    "            print(f\"  ðŸ“¥ Mode: Incremental (data.parquet + {len(date_files)} date files)\")\n",
    "            \n",
    "        elif has_existing_data:\n",
    "            # Only data.parquet exists (already consolidated, no new files)\n",
    "            merge_sql = f\"SELECT * FROM read_parquet('{data_parquet_path}')\"\n",
    "            result['files_read'] = 1\n",
    "            print(f\"  ðŸ“¥ Mode: Read existing data.parquet only\")\n",
    "            \n",
    "        else:\n",
    "            # Only date files exist (full consolidation needed)\n",
    "            date_paths = [f\"s3://{S3_BUCKET}/{f['key']}\" for f in date_files]\n",
    "            paths_str = \"', '\".join(date_paths)\n",
    "            \n",
    "            merge_sql = f\"SELECT * FROM read_parquet(['{paths_str}'])\"\n",
    "            result['files_read'] = len(date_files)\n",
    "            print(f\"  ðŸ“¥ Mode: Full consolidation from {len(date_files)} date files\")\n",
    "        \n",
    "        # Execute read\n",
    "        df = conn.execute(merge_sql).fetchdf()\n",
    "        read_time = time.time() - read_start\n",
    "        result['read_time_s'] = read_time\n",
    "        \n",
    "        # Deduplicate\n",
    "        df = df.drop_duplicates()\n",
    "        result['rows_consolidated'] = len(df)\n",
    "        \n",
    "        print(f\"  âœ… Read {len(df):,} rows in {read_time:.2f}s\")\n",
    "        \n",
    "        # Write to S3 (unless dry_run)\n",
    "        if not dry_run:\n",
    "            write_start = time.time()\n",
    "            \n",
    "            buffer = BytesIO()\n",
    "            table = pa.Table.from_pandas(df)\n",
    "            pq.write_table(table, buffer, compression='snappy')\n",
    "            buffer.seek(0)\n",
    "            \n",
    "            s3_client.put_object(\n",
    "                Bucket=S3_BUCKET,\n",
    "                Key=f\"{S3_PREFIX}/symbol={symbol}/data.parquet\",\n",
    "                Body=buffer.getvalue()\n",
    "            )\n",
    "            \n",
    "            write_time = time.time() - write_start\n",
    "            result['write_time_s'] = write_time\n",
    "            print(f\"  âœ… Wrote to S3 in {write_time:.2f}s\")\n",
    "        else:\n",
    "            print(f\"  â¸ï¸  Dry run - skipped S3 write\")\n",
    "        \n",
    "        result['total_time_s'] = time.time() - total_start\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['status'] = 'failed'\n",
    "        result['error'] = str(e)\n",
    "        result['total_time_s'] = time.time() - total_start\n",
    "        print(f\"  âŒ Error: {e}\")\n",
    "        return result, df\n",
    "\n",
    "\n",
    "# Test with a single symbol\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ§ª TEST 1: Single Symbol Consolidation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Pick a symbol from vacuumed range\n",
    "test_symbol = vacuumed_symbols[0] if vacuumed_symbols else 'AAPL'\n",
    "print(f\"\\nðŸ“Š Testing symbol: {test_symbol}\")\n",
    "\n",
    "# Initialize DuckDB\n",
    "conn = init_duckdb()\n",
    "\n",
    "# Run consolidation (dry run - no write)\n",
    "result = consolidate_symbol(conn, test_symbol, dry_run=True)\n",
    "\n",
    "print(f\"\\nðŸ“Š Result:\")\n",
    "for key, value in result.items():\n",
    "    print(f\"   {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919e5ef",
   "metadata": {},
   "source": [
    "## Test 4b: Batch Consolidation (10 Symbols - Sequential)\n",
    "\n",
    "Tests batch consolidation performance with sequential processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a307d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ§ª TEST 2: Batch Consolidation (10 Symbols)\n",
      "================================================================================\n",
      "\n",
      "ðŸ“‹ Testing 10 symbols: ['A', 'AA', 'AACB', 'AACG', 'AACI', 'AACT', 'AAL', 'AAM', 'AAME', 'AAMI']\n",
      "âœ… DuckDB initialized with AWS credentials\n",
      "\n",
      "[1/10] Processing A...\n",
      "  ðŸ“ data.parquet exists: True\n",
      "  ðŸ“… date=*.parquet files: 24\n",
      "  ðŸ“¥ Mode: Incremental (data.parquet + 24 date files)\n",
      "  âœ… Read 6,532 rows in 0.61s\n",
      "  â¸ï¸  Dry run - skipped S3 write\n",
      "\n",
      "[2/10] Processing AA...\n",
      "  ðŸ“ data.parquet exists: True\n",
      "  ðŸ“… date=*.parquet files: 24\n",
      "  ðŸ“¥ Mode: Incremental (data.parquet + 24 date files)\n",
      "  âœ… Read 16,071 rows in 0.69s\n",
      "  â¸ï¸  Dry run - skipped S3 write\n",
      "\n",
      "[3/10] Processing AACB...\n",
      "  ðŸ“ data.parquet exists: True\n",
      "  ðŸ“… date=*.parquet files: 16\n",
      "  ðŸ“¥ Mode: Incremental (data.parquet + 16 date files)\n",
      "  âœ… Read 136 rows in 0.48s\n",
      "  â¸ï¸  Dry run - skipped S3 write\n",
      "\n",
      "[4/10] Processing AACG...\n",
      "  ðŸ“ data.parquet exists: True\n",
      "  ðŸ“… date=*.parquet files: 24\n",
      "  ðŸ“¥ Mode: Incremental (data.parquet + 24 date files)\n",
      "  âœ… Read 4,474 rows in 0.67s\n",
      "  â¸ï¸  Dry run - skipped S3 write\n",
      "\n",
      "[5/10] Processing AACI...\n",
      "  ðŸ“ data.parquet exists: True\n",
      "  ðŸ“… date=*.parquet files: 0\n",
      "  ðŸ“¥ Mode: Read existing data.parquet only\n",
      "  âœ… Read 72 rows in 0.09s\n",
      "  â¸ï¸  Dry run - skipped S3 write\n",
      "\n",
      "[6/10] Processing AACT...\n",
      "  ðŸ“ data.parquet exists: True\n",
      "  ðŸ“… date=*.parquet files: 0\n",
      "  ðŸ“¥ Mode: Read existing data.parquet only\n",
      "  âœ… Read 573 rows in 0.17s\n",
      "  â¸ï¸  Dry run - skipped S3 write\n",
      "\n",
      "[7/10] Processing AAL...\n",
      "  ðŸ“ data.parquet exists: True\n",
      "  ðŸ“… date=*.parquet files: 24\n",
      "  ðŸ“¥ Mode: Incremental (data.parquet + 24 date files)\n",
      "  âœ… Read 5,061 rows in 0.73s\n",
      "  â¸ï¸  Dry run - skipped S3 write\n",
      "\n",
      "[8/10] Processing AAM...\n",
      "  ðŸ“ data.parquet exists: True\n",
      "  ðŸ“… date=*.parquet files: 23\n",
      "  ðŸ“¥ Mode: Incremental (data.parquet + 23 date files)\n",
      "  âœ… Read 286 rows in 0.51s\n",
      "  â¸ï¸  Dry run - skipped S3 write\n",
      "\n",
      "[9/10] Processing AAME...\n",
      "  ðŸ“ data.parquet exists: True\n",
      "  ðŸ“… date=*.parquet files: 24\n",
      "  ðŸ“¥ Mode: Incremental (data.parquet + 24 date files)\n",
      "  âœ… Read 11,506 rows in 0.63s\n",
      "  â¸ï¸  Dry run - skipped S3 write\n",
      "\n",
      "[10/10] Processing AAMI...\n",
      "  ðŸ“ data.parquet exists: True\n",
      "  ðŸ“… date=*.parquet files: 24\n",
      "  ðŸ“¥ Mode: Incremental (data.parquet + 24 date files)\n",
      "  âœ… Read 2,787 rows in 0.70s\n",
      "  â¸ï¸  Dry run - skipped S3 write\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š BATCH CONSOLIDATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ˆ Overall Statistics:\n",
      "   Total symbols: 10\n",
      "   Successful: 10\n",
      "   Skipped: 0\n",
      "   Failed: 0\n",
      "   Total time: 5.76s\n",
      "   Avg time per symbol: 0.58s\n",
      "\n",
      "ðŸ“Š Read Performance:\n",
      "   Total rows read: 47,498\n",
      "   Total files read: 193\n",
      "   Avg read time: 0.53s\n",
      "\n",
      "ðŸ“ File Status:\n",
      "   Symbols with data.parquet: 10\n",
      "   Symbols with date files: 8\n",
      "\n",
      "ðŸ“Š Detailed Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>status</th>\n",
       "      <th>rows_consolidated</th>\n",
       "      <th>files_read</th>\n",
       "      <th>read_time_s</th>\n",
       "      <th>has_data_parquet</th>\n",
       "      <th>date_files_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>success</td>\n",
       "      <td>6532</td>\n",
       "      <td>25</td>\n",
       "      <td>0.607652</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA</td>\n",
       "      <td>success</td>\n",
       "      <td>16071</td>\n",
       "      <td>25</td>\n",
       "      <td>0.685474</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AACB</td>\n",
       "      <td>success</td>\n",
       "      <td>136</td>\n",
       "      <td>17</td>\n",
       "      <td>0.475547</td>\n",
       "      <td>True</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AACG</td>\n",
       "      <td>success</td>\n",
       "      <td>4474</td>\n",
       "      <td>25</td>\n",
       "      <td>0.667722</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AACI</td>\n",
       "      <td>success</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>0.094304</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AACT</td>\n",
       "      <td>success</td>\n",
       "      <td>573</td>\n",
       "      <td>1</td>\n",
       "      <td>0.169161</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AAL</td>\n",
       "      <td>success</td>\n",
       "      <td>5061</td>\n",
       "      <td>25</td>\n",
       "      <td>0.725840</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AAM</td>\n",
       "      <td>success</td>\n",
       "      <td>286</td>\n",
       "      <td>24</td>\n",
       "      <td>0.513023</td>\n",
       "      <td>True</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AAME</td>\n",
       "      <td>success</td>\n",
       "      <td>11506</td>\n",
       "      <td>25</td>\n",
       "      <td>0.625268</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AAMI</td>\n",
       "      <td>success</td>\n",
       "      <td>2787</td>\n",
       "      <td>25</td>\n",
       "      <td>0.696116</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol   status  rows_consolidated  files_read  read_time_s  \\\n",
       "0      A  success               6532          25     0.607652   \n",
       "1     AA  success              16071          25     0.685474   \n",
       "2   AACB  success                136          17     0.475547   \n",
       "3   AACG  success               4474          25     0.667722   \n",
       "4   AACI  success                 72           1     0.094304   \n",
       "5   AACT  success                573           1     0.169161   \n",
       "6    AAL  success               5061          25     0.725840   \n",
       "7    AAM  success                286          24     0.513023   \n",
       "8   AAME  success              11506          25     0.625268   \n",
       "9   AAMI  success               2787          25     0.696116   \n",
       "\n",
       "   has_data_parquet  date_files_count  \n",
       "0              True                24  \n",
       "1              True                24  \n",
       "2              True                16  \n",
       "3              True                24  \n",
       "4              True                 0  \n",
       "5              True                 0  \n",
       "6              True                24  \n",
       "7              True                23  \n",
       "8              True                24  \n",
       "9              True                24  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test batch consolidation for multiple symbols\n",
    "Same as Lambda would do in production\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ§ª TEST 2: Batch Consolidation (10 Symbols)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select 10 symbols from vacuumed range\n",
    "batch_symbols = vacuumed_symbols[:10] if len(vacuumed_symbols) >= 10 else vacuumed_symbols\n",
    "print(f\"\\nðŸ“‹ Testing {len(batch_symbols)} symbols: {batch_symbols}\")\n",
    "\n",
    "# Initialize DuckDB connection (reuse for all symbols)\n",
    "conn_batch = init_duckdb()\n",
    "\n",
    "# Track results\n",
    "batch_results = []\n",
    "batch_start = time.time()\n",
    "\n",
    "for i, symbol in enumerate(batch_symbols, 1):\n",
    "    print(f\"\\n[{i}/{len(batch_symbols)}] Processing {symbol}...\")\n",
    "    result = consolidate_symbol(conn_batch, symbol, dry_run=True)\n",
    "    batch_results.append(result)\n",
    "\n",
    "batch_time = time.time() - batch_start\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“Š BATCH CONSOLIDATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "successful = [r for r in batch_results if r['status'] == 'success']\n",
    "skipped = [r for r in batch_results if r['status'] == 'skipped']\n",
    "failed = [r for r in batch_results if r['status'] == 'failed']\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Overall Statistics:\")\n",
    "print(f\"   Total symbols: {len(batch_results)}\")\n",
    "print(f\"   Successful: {len(successful)}\")\n",
    "print(f\"   Skipped: {len(skipped)}\")\n",
    "print(f\"   Failed: {len(failed)}\")\n",
    "print(f\"   Total time: {batch_time:.2f}s\")\n",
    "print(f\"   Avg time per symbol: {batch_time/len(batch_results):.2f}s\")\n",
    "\n",
    "if successful:\n",
    "    total_rows = sum(r['rows_consolidated'] for r in successful)\n",
    "    total_files = sum(r['files_read'] for r in successful)\n",
    "    avg_read_time = sum(r['read_time_s'] for r in successful) / len(successful)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Read Performance:\")\n",
    "    print(f\"   Total rows read: {total_rows:,}\")\n",
    "    print(f\"   Total files read: {total_files}\")\n",
    "    print(f\"   Avg read time: {avg_read_time:.2f}s\")\n",
    "    \n",
    "    # Check how many have data.parquet vs date files only\n",
    "    with_data_parquet = sum(1 for r in successful if r['has_data_parquet'])\n",
    "    with_date_files = sum(1 for r in successful if r['date_files_count'] > 0)\n",
    "    \n",
    "    print(f\"\\nðŸ“ File Status:\")\n",
    "    print(f\"   Symbols with data.parquet: {with_data_parquet}\")\n",
    "    print(f\"   Symbols with date files: {with_date_files}\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(batch_results)\n",
    "print(\"\\nðŸ“Š Detailed Results:\")\n",
    "display(summary_df[['symbol', 'status', 'rows_consolidated', 'files_read', 'read_time_s', 'has_data_parquet', 'date_files_count']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd91e22",
   "metadata": {},
   "source": [
    "## Test 4c: Runtime Estimation (Sequential)\n",
    "\n",
    "Estimates how long it would take to consolidate all 5,350 symbols based on test results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3045a22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ“Š LAMBDA RUNTIME ESTIMATION\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ˆ Performance Metrics (from test):\n",
      "   Avg time per symbol: 0.58s\n",
      "   Symbols with data.parquet: 10/10\n",
      "\n",
      "â±ï¸  Lambda Capacity Estimation:\n",
      "   Lambda timeout: 900s (15 min)\n",
      "   Available time (after overhead): 870s\n",
      "   Estimated symbols per Lambda run: ~1509\n",
      "\n",
      "ðŸš€ Full Consolidation Estimate:\n",
      "   Total symbols: 5,350\n",
      "   Lambda runs needed: 4\n",
      "   Total time: ~51.4 minutes\n",
      "\n",
      "ðŸ’¡ Daily Incremental Consolidation:\n",
      "   If only ~100 symbols have new data per day:\n",
      "   Estimated time: ~58s (1.0 min)\n",
      "   âœ… Fits well within 15 min Lambda timeout!\n",
      "\n",
      "ðŸ“¦ Recommendation:\n",
      "   âœ… Lambda is suitable for daily incremental consolidation\n",
      "   âœ… Use AWS Batch only for initial full consolidation\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Estimate Lambda runtime based on test results\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ“Š LAMBDA RUNTIME ESTIMATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if successful:\n",
    "    avg_time_per_symbol = batch_time / len(batch_results)\n",
    "    \n",
    "    # Lambda limits\n",
    "    LAMBDA_TIMEOUT = 15 * 60  # 15 minutes in seconds\n",
    "    LAMBDA_OVERHEAD = 30  # seconds for cold start + cleanup\n",
    "    \n",
    "    # Estimate symbols per Lambda run\n",
    "    available_time = LAMBDA_TIMEOUT - LAMBDA_OVERHEAD\n",
    "    symbols_per_run = int(available_time / avg_time_per_symbol)\n",
    "    \n",
    "    # Total symbols to process (assuming ~5,350 symbols)\n",
    "    TOTAL_SYMBOLS = 5350\n",
    "    lambda_runs_needed = (TOTAL_SYMBOLS + symbols_per_run - 1) // symbols_per_run\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Performance Metrics (from test):\")\n",
    "    print(f\"   Avg time per symbol: {avg_time_per_symbol:.2f}s\")\n",
    "    print(f\"   Symbols with data.parquet: {with_data_parquet}/{len(successful)}\")\n",
    "    \n",
    "    print(f\"\\nâ±ï¸  Lambda Capacity Estimation:\")\n",
    "    print(f\"   Lambda timeout: {LAMBDA_TIMEOUT}s (15 min)\")\n",
    "    print(f\"   Available time (after overhead): {available_time}s\")\n",
    "    print(f\"   Estimated symbols per Lambda run: ~{symbols_per_run}\")\n",
    "    \n",
    "    print(f\"\\nðŸš€ Full Consolidation Estimate:\")\n",
    "    print(f\"   Total symbols: {TOTAL_SYMBOLS:,}\")\n",
    "    print(f\"   Lambda runs needed: {lambda_runs_needed}\")\n",
    "    print(f\"   Total time: ~{(TOTAL_SYMBOLS * avg_time_per_symbol) / 60:.1f} minutes\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Daily Incremental Consolidation:\")\n",
    "    print(f\"   If only ~100 symbols have new data per day:\")\n",
    "    estimated_daily_time = 100 * avg_time_per_symbol\n",
    "    print(f\"   Estimated time: ~{estimated_daily_time:.0f}s ({estimated_daily_time/60:.1f} min)\")\n",
    "    print(f\"   âœ… Fits well within 15 min Lambda timeout!\")\n",
    "    \n",
    "    print(f\"\\nðŸ“¦ Recommendation:\")\n",
    "    if avg_time_per_symbol < 3:\n",
    "        print(f\"   âœ… Lambda is suitable for daily incremental consolidation\")\n",
    "        print(f\"   âœ… Use AWS Batch only for initial full consolidation\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ Consider optimizations or AWS Batch for large workloads\")\n",
    "else:\n",
    "    print(\"âŒ No successful results to estimate from\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6870f7",
   "metadata": {},
   "source": [
    "## Test 4d: ðŸš€ Parallel Consolidation (ThreadPoolExecutor)\n",
    "\n",
    "**Goal**: Compare sequential vs parallel consolidation performance\n",
    "\n",
    "**Approach**: Use `concurrent.futures.ThreadPoolExecutor` to process multiple symbols simultaneously\n",
    "\n",
    "**Key Design Decisions**:\n",
    "- Each thread gets its own DuckDB connection (thread-safe)\n",
    "- Similar to async pattern used in `daily_ohlcv_fetcher.py`\n",
    "- Optimal workers: 5-10 (balances parallelism vs overhead)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "968c32ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Parallel consolidation functions defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parallel Consolidation using ThreadPoolExecutor\n",
    "Similar to the async pattern in daily_ohlcv_fetcher.py\n",
    "\"\"\"\n",
    "\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "\n",
    "def consolidate_symbol_thread_safe(symbol: str, dry_run: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Thread-safe consolidation function.\n",
    "    Each thread creates its own DuckDB connection.\n",
    "    \"\"\"\n",
    "    # Create new DuckDB connection for this thread\n",
    "    thread_conn = duckdb.connect(':memory:')\n",
    "    thread_conn.execute(\"INSTALL httpfs;\")\n",
    "    thread_conn.execute(\"LOAD httpfs;\")\n",
    "    thread_conn.execute(f\"SET s3_region='{AWS_REGION}'\")\n",
    "    \n",
    "    aws_access_key = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "    aws_secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "    \n",
    "    if aws_access_key and aws_secret_key:\n",
    "        thread_conn.execute(f\"SET s3_access_key_id='{aws_access_key}'\")\n",
    "        thread_conn.execute(f\"SET s3_secret_access_key='{aws_secret_key}'\")\n",
    "    \n",
    "    result = {\n",
    "        'symbol': symbol,\n",
    "        'status': 'success',\n",
    "        'rows_consolidated': 0,\n",
    "        'files_read': 0,\n",
    "        'read_time_s': 0,\n",
    "        'total_time_s': 0,\n",
    "        'has_data_parquet': False,\n",
    "        'date_files_count': 0\n",
    "    }\n",
    "    \n",
    "    total_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        s3_symbol_path = f\"s3://{S3_BUCKET}/{S3_PREFIX}/symbol={symbol}\"\n",
    "        data_parquet_path = f\"{s3_symbol_path}/data.parquet\"\n",
    "        \n",
    "        # Check current state\n",
    "        has_existing_data = check_data_parquet_exists(symbol)\n",
    "        date_files = list_date_files(symbol)\n",
    "        \n",
    "        result['has_data_parquet'] = has_existing_data\n",
    "        result['date_files_count'] = len(date_files)\n",
    "        \n",
    "        if not date_files and not has_existing_data:\n",
    "            result['status'] = 'skipped'\n",
    "            result['reason'] = 'No files found'\n",
    "            result['total_time_s'] = time.time() - total_start\n",
    "            return result\n",
    "        \n",
    "        # Build SQL query based on what files exist\n",
    "        read_start = time.time()\n",
    "        \n",
    "        if has_existing_data and date_files:\n",
    "            date_paths = [f\"s3://{S3_BUCKET}/{f['key']}\" for f in date_files]\n",
    "            paths_str = \"', '\".join(date_paths)\n",
    "            merge_sql = f\"\"\"\n",
    "                SELECT * FROM read_parquet('{data_parquet_path}')\n",
    "                UNION ALL\n",
    "                SELECT * FROM read_parquet(['{paths_str}'])\n",
    "            \"\"\"\n",
    "            result['files_read'] = 1 + len(date_files)\n",
    "        elif has_existing_data:\n",
    "            merge_sql = f\"SELECT * FROM read_parquet('{data_parquet_path}')\"\n",
    "            result['files_read'] = 1\n",
    "        else:\n",
    "            date_paths = [f\"s3://{S3_BUCKET}/{f['key']}\" for f in date_files]\n",
    "            paths_str = \"', '\".join(date_paths)\n",
    "            merge_sql = f\"SELECT * FROM read_parquet(['{paths_str}'])\"\n",
    "            result['files_read'] = len(date_files)\n",
    "        \n",
    "        # Execute read\n",
    "        df = thread_conn.execute(merge_sql).fetchdf()\n",
    "        read_time = time.time() - read_start\n",
    "        result['read_time_s'] = read_time\n",
    "        \n",
    "        # Deduplicate\n",
    "        df = df.drop_duplicates()\n",
    "        result['rows_consolidated'] = len(df)\n",
    "        \n",
    "        # Write to S3 (unless dry_run)\n",
    "        if not dry_run:\n",
    "            buffer = BytesIO()\n",
    "            table = pa.Table.from_pandas(df)\n",
    "            pq.write_table(table, buffer, compression='snappy')\n",
    "            buffer.seek(0)\n",
    "            \n",
    "            s3_client.put_object(\n",
    "                Bucket=S3_BUCKET,\n",
    "                Key=f\"{S3_PREFIX}/symbol={symbol}/data.parquet\",\n",
    "                Body=buffer.getvalue()\n",
    "            )\n",
    "        \n",
    "        result['total_time_s'] = time.time() - total_start\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['status'] = 'failed'\n",
    "        result['error'] = str(e)\n",
    "        result['total_time_s'] = time.time() - total_start\n",
    "    finally:\n",
    "        thread_conn.close()\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def consolidate_parallel(\n",
    "    symbols: List[str],\n",
    "    max_workers: int = 10,\n",
    "    dry_run: bool = True\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Consolidate multiple symbols in parallel using ThreadPoolExecutor\n",
    "    \n",
    "    Args:\n",
    "        symbols: List of symbols to consolidate\n",
    "        max_workers: Number of parallel threads\n",
    "        dry_run: If True, don't write to S3\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (results list, total_time)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"ðŸš€ Starting parallel consolidation with {max_workers} workers...\")\n",
    "    print(f\"ðŸ“‹ Processing {len(symbols)} symbols\")\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_symbol = {\n",
    "            executor.submit(consolidate_symbol_thread_safe, sym, dry_run): sym\n",
    "            for sym in symbols\n",
    "        }\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        completed = 0\n",
    "        for future in concurrent.futures.as_completed(future_to_symbol):\n",
    "            symbol = future_to_symbol[future]\n",
    "            completed += 1\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                \n",
    "                status_icon = \"âœ…\" if result['status'] == 'success' else \"â­ï¸\" if result['status'] == 'skipped' else \"âŒ\"\n",
    "                print(f\"[{completed}/{len(symbols)}] {status_icon} {symbol}: {result['rows_consolidated']:,} rows in {result['total_time_s']:.2f}s\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[{completed}/{len(symbols)}] âŒ {symbol}: {e}\")\n",
    "                results.append({\n",
    "                    'symbol': symbol,\n",
    "                    'status': 'failed',\n",
    "                    'error': str(e)\n",
    "                })\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    return results, total_time\n",
    "\n",
    "print(\"âœ… Parallel consolidation functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a15cfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ§ª TEST 4a: Sequential vs Parallel Consolidation Comparison\n",
      "================================================================================\n",
      "\n",
      "ðŸ“‹ Testing with 20 symbols\n",
      "   Symbols: ['A', 'AA', 'AACB', 'AACG', 'AACI']... (showing first 5)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Š SEQUENTIAL Processing (1 symbol at a time)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… DuckDB initialized with AWS credentials\n",
      "   Progress: 5/20 symbols processed...\n",
      "   Progress: 10/20 symbols processed...\n",
      "   Progress: 15/20 symbols processed...\n",
      "   Progress: 20/20 symbols processed...\n",
      "\n",
      "âœ… Sequential completed in 13.19s\n",
      "   Avg time per symbol: 0.66s\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Š PARALLEL Processing (5 workers)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸš€ Starting parallel consolidation with 5 workers...\n",
      "ðŸ“‹ Processing 20 symbols\n",
      "[1/20] âœ… AACI: 72 rows in 0.17s\n",
      "[2/20] âœ… AACT: 573 rows in 0.21s\n",
      "[3/20] âœ… AACB: 136 rows in 0.51s\n",
      "[4/20] âœ… AA: 16,071 rows in 0.65s\n",
      "[5/20] âœ… A: 6,532 rows in 0.67s\n",
      "[6/20] âœ… AACG: 4,474 rows in 0.73s\n",
      "[7/20] âœ… AAM: 286 rows in 0.53s\n",
      "[8/20] âœ… AAL: 5,061 rows in 0.70s\n",
      "[9/20] âœ… AAMI: 2,787 rows in 0.67s\n",
      "[10/20] âœ… AAME: 11,506 rows in 0.75s\n",
      "[11/20] âœ… AAOI: 3,048 rows in 0.76s\n",
      "[12/20] âœ… AAON: 8,281 rows in 0.62s\n",
      "[13/20] âœ… AAP: 6,024 rows in 0.70s\n",
      "[14/20] âœ… AAPG: 197 rows in 0.55s\n",
      "[15/20] âœ… AARD: 185 rows in 0.59s\n",
      "[16/20] âœ… AAPL: 11,318 rows in 0.75s\n",
      "[17/20] âœ… AAT: 3,727 rows in 0.64s\n",
      "[18/20] âœ… AAUC: 309 rows in 0.58s\n",
      "[19/20] âœ… AB: 9,464 rows in 0.68s\n",
      "[20/20] âœ… ABAT: 2,442 rows in 0.65s\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Š PARALLEL Processing (10 workers)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸš€ Starting parallel consolidation with 10 workers...\n",
      "ðŸ“‹ Processing 20 symbols\n",
      "[1/20] âœ… AACI: 72 rows in 0.15s\n",
      "[2/20] âœ… AACT: 573 rows in 0.24s\n",
      "[3/20] âœ… AACB: 136 rows in 0.60s\n",
      "[4/20] âœ… AAM: 286 rows in 0.76s\n",
      "[5/20] âœ… AAL: 5,061 rows in 0.92s\n",
      "[6/20] âœ… AAMI: 2,787 rows in 1.01s\n",
      "[7/20] âœ… AAME: 11,506 rows in 1.02s\n",
      "[8/20] âœ… A: 6,532 rows in 1.03s\n",
      "[9/20] âœ… AA: 16,071 rows in 1.06s\n",
      "[10/20] âœ… AACG: 4,474 rows in 1.04s\n",
      "[11/20] âœ… AAOI: 3,048 rows in 0.94s\n",
      "[12/20] âœ… AAON: 8,281 rows in 0.99s\n",
      "[13/20] âœ… AAPG: 197 rows in 0.54s\n",
      "[14/20] âœ… AAP: 6,024 rows in 0.94s\n",
      "[15/20] âœ… AARD: 185 rows in 0.60s\n",
      "[16/20] âœ… AAPL: 11,318 rows in 0.69s\n",
      "[17/20] âœ… AAT: 3,727 rows in 0.66s\n",
      "[18/20] âœ… AAUC: 309 rows in 0.64s\n",
      "[19/20] âœ… ABAT: 2,442 rows in 0.72s\n",
      "[20/20] âœ… AB: 9,464 rows in 0.76s\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š PERFORMANCE COMPARISON SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Method                    Time (s)     Avg/Symbol   Speedup   \n",
      "------------------------------------------------------------\n",
      "Sequential (1 worker)     13.19        0.66         1.0x      \n",
      "Parallel (5 workers)      2.81         0.14         4.7       x\n",
      "Parallel (10 workers)     1.91         0.10         6.9       x\n",
      "\n",
      "ðŸ† WINNER: Parallel (10 workers)\n",
      "   Speedup: 6.9x faster than sequential!\n",
      "   Time saved: 11.28s (85.5%)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ðŸ§ª Test 4a: Sequential vs Parallel Comparison (20 symbols)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ§ª TEST 4a: Sequential vs Parallel Consolidation Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use 20 symbols for a meaningful comparison\n",
    "test_count = 20\n",
    "comparison_symbols = vacuumed_symbols[:test_count] if len(vacuumed_symbols) >= test_count else vacuumed_symbols\n",
    "print(f\"\\nðŸ“‹ Testing with {len(comparison_symbols)} symbols\")\n",
    "print(f\"   Symbols: {comparison_symbols[:5]}... (showing first 5)\")\n",
    "\n",
    "# ============================================\n",
    "# SEQUENTIAL TEST\n",
    "# ============================================\n",
    "print(\"\\n\" + \"â”€\" * 80)\n",
    "print(\"ðŸ“Š SEQUENTIAL Processing (1 symbol at a time)\")\n",
    "print(\"â”€\" * 80)\n",
    "\n",
    "seq_conn = init_duckdb()\n",
    "seq_results = []\n",
    "seq_start = time.time()\n",
    "\n",
    "for i, symbol in enumerate(comparison_symbols, 1):\n",
    "    result = consolidate_symbol_thread_safe(symbol, dry_run=True)\n",
    "    seq_results.append(result)\n",
    "    if i % 5 == 0:\n",
    "        print(f\"   Progress: {i}/{len(comparison_symbols)} symbols processed...\")\n",
    "\n",
    "seq_time = time.time() - seq_start\n",
    "seq_successful = [r for r in seq_results if r['status'] == 'success']\n",
    "\n",
    "print(f\"\\nâœ… Sequential completed in {seq_time:.2f}s\")\n",
    "print(f\"   Avg time per symbol: {seq_time/len(comparison_symbols):.2f}s\")\n",
    "\n",
    "# ============================================\n",
    "# PARALLEL TEST (5 workers)\n",
    "# ============================================\n",
    "print(\"\\n\" + \"â”€\" * 80)\n",
    "print(\"ðŸ“Š PARALLEL Processing (5 workers)\")\n",
    "print(\"â”€\" * 80)\n",
    "\n",
    "parallel_results_5, parallel_time_5 = consolidate_parallel(\n",
    "    symbols=comparison_symbols,\n",
    "    max_workers=5,\n",
    "    dry_run=True\n",
    ")\n",
    "\n",
    "parallel_successful_5 = [r for r in parallel_results_5 if r['status'] == 'success']\n",
    "\n",
    "# ============================================\n",
    "# PARALLEL TEST (10 workers)\n",
    "# ============================================\n",
    "print(\"\\n\" + \"â”€\" * 80)\n",
    "print(\"ðŸ“Š PARALLEL Processing (10 workers)\")\n",
    "print(\"â”€\" * 80)\n",
    "\n",
    "parallel_results_10, parallel_time_10 = consolidate_parallel(\n",
    "    symbols=comparison_symbols,\n",
    "    max_workers=10,\n",
    "    dry_run=True\n",
    ")\n",
    "\n",
    "parallel_successful_10 = [r for r in parallel_results_10 if r['status'] == 'success']\n",
    "\n",
    "# ============================================\n",
    "# COMPARISON SUMMARY\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“Š PERFORMANCE COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n{'Method':<25} {'Time (s)':<12} {'Avg/Symbol':<12} {'Speedup':<10}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Sequential (1 worker)':<25} {seq_time:<12.2f} {seq_time/len(comparison_symbols):<12.2f} {'1.0x':<10}\")\n",
    "print(f\"{'Parallel (5 workers)':<25} {parallel_time_5:<12.2f} {parallel_time_5/len(comparison_symbols):<12.2f} {seq_time/parallel_time_5:<10.1f}x\")\n",
    "print(f\"{'Parallel (10 workers)':<25} {parallel_time_10:<12.2f} {parallel_time_10/len(comparison_symbols):<12.2f} {seq_time/parallel_time_10:<10.1f}x\")\n",
    "\n",
    "best_parallel_time = min(parallel_time_5, parallel_time_10)\n",
    "best_workers = 5 if parallel_time_5 < parallel_time_10 else 10\n",
    "speedup = seq_time / best_parallel_time\n",
    "\n",
    "print(f\"\\nðŸ† WINNER: Parallel ({best_workers} workers)\")\n",
    "print(f\"   Speedup: {speedup:.1f}x faster than sequential!\")\n",
    "print(f\"   Time saved: {seq_time - best_parallel_time:.2f}s ({(seq_time - best_parallel_time)/seq_time*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dd637a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ§ª TEST 4b: Large Scale Parallel Consolidation (50 symbols)\n",
      "================================================================================\n",
      "\n",
      "ðŸ“‹ Testing with 50 symbols using 10 workers\n",
      "ðŸš€ Starting parallel consolidation with 10 workers...\n",
      "ðŸ“‹ Processing 50 symbols\n",
      "[1/50] âœ… AACI: 72 rows in 0.22s\n",
      "[2/50] âœ… AACT: 573 rows in 0.25s\n",
      "[3/50] âœ… AACB: 136 rows in 0.54s\n",
      "[4/50] âœ… AAM: 286 rows in 0.70s\n",
      "[5/50] âœ… AACG: 4,474 rows in 0.90s\n",
      "[6/50] âœ… AAMI: 2,787 rows in 0.96s\n",
      "[7/50] âœ… A: 6,532 rows in 0.96s\n",
      "[8/50] âœ… AAL: 5,061 rows in 0.96s\n",
      "[9/50] âœ… AAME: 11,506 rows in 1.00s\n",
      "[10/50] âœ… AAOI: 3,048 rows in 0.83s\n",
      "[11/50] âœ… AA: 16,071 rows in 1.21s\n",
      "[12/50] âœ… AAON: 8,281 rows in 1.01s\n",
      "[13/50] âœ… AAPG: 197 rows in 0.66s\n",
      "[14/50] âœ… AAP: 6,024 rows in 0.98s\n",
      "[15/50] âœ… AAUC: 309 rows in 0.63s\n",
      "[16/50] âœ… AARD: 185 rows in 0.64s\n",
      "[17/50] âœ… AAPL: 11,318 rows in 0.81s\n",
      "[18/50] âœ… AAT: 3,727 rows in 0.77s\n",
      "[19/50] âœ… ABAT: 2,442 rows in 0.74s\n",
      "[20/50] âœ… AB: 9,464 rows in 0.82s\n",
      "[21/50] âœ… ABBV: 3,233 rows in 0.83s\n",
      "[22/50] âœ… ABCB: 7,922 rows in 0.81s\n",
      "[23/50] âœ… ABCL: 1,232 rows in 0.78s\n",
      "[24/50] âœ… ABEO: 11,376 rows in 0.87s\n",
      "[25/50] âœ… ABG: 5,948 rows in 0.76s\n",
      "[26/50] âœ… ABEV: 7,216 rows in 0.80s\n",
      "[27/50] âœ… ABLLL: 490 rows in 0.73s\n",
      "[28/50] âœ… ABL: 1,295 rows in 0.84s\n",
      "[29/50] âœ… ABM: 11,506 rows in 0.81s\n",
      "[30/50] âœ… ABLV: 793 rows in 0.82s\n",
      "[31/50] âœ… ABNB: 1,233 rows in 0.72s\n",
      "[32/50] âœ… ABOS: 1,094 rows in 0.79s\n",
      "[33/50] âœ… ABP: 921 rows in 0.72s\n",
      "[34/50] âœ… ABSI: 1,080 rows in 0.81s\n",
      "[35/50] âœ… ABT: 11,506 rows in 0.92s\n",
      "[36/50] âœ… ABTC: 1,937 rows in 0.85s\n",
      "[37/50] âœ… ABR: 5,432 rows in 0.97s\n",
      "[38/50] âœ… ABTS: 2,913 rows in 0.82s\n",
      "[39/50] âœ… ABVE: 340 rows in 0.73s\n",
      "[40/50] âœ… ABVC: 5,282 rows in 0.91s\n",
      "[41/50] âœ… ABUS: 4,602 rows in 0.95s\n",
      "[42/50] âœ… ABVX: 514 rows in 0.82s\n",
      "[43/50] âœ… ACA: 1,761 rows in 1.02s\n",
      "[44/50] âœ… ACAD: 5,397 rows in 0.86s\n",
      "[45/50] âœ… ACB: 2,848 rows in 0.85s\n",
      "[46/50] âœ… ACCO: 5,089 rows in 0.88s\n",
      "[47/50] âœ… ACEL: 2,063 rows in 0.87s\n",
      "[48/50] âœ… ACDC: 875 rows in 1.00s\n",
      "[49/50] âœ… ACFN: 8,496 rows in 0.80s\n",
      "[50/50] âœ… ACET: 1,957 rows in 0.90s\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š LARGE SCALE TEST RESULTS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ˆ Performance Metrics:\n",
      "   Total symbols: 50\n",
      "   Successful: 50\n",
      "   Failed: 0\n",
      "   Total time: 4.51s\n",
      "   Throughput: 11.1 symbols/sec\n",
      "   Total rows processed: 208,844\n",
      "   Rows per second: 46,357\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š UPDATED LAMBDA ESTIMATION (with Parallel)\n",
      "================================================================================\n",
      "\n",
      "âš¡ With Parallel Processing (10 workers):\n",
      "   Throughput: 11.1 symbols/sec\n",
      "   Symbols per Lambda run: ~9,655\n",
      "   Lambda runs needed for full consolidation: 1\n",
      "\n",
      "ðŸš€ Full Consolidation Estimate:\n",
      "   Time for all 5,350 symbols: ~8.0 min\n",
      "\n",
      "ðŸ’¡ Daily Incremental (100 symbols with new data):\n",
      "   Estimated time: ~9s (0.2 min)\n",
      "   âœ… Fits easily within 15 min Lambda timeout!\n",
      "\n",
      "ðŸ“¦ RECOMMENDATION:\n",
      "   âœ… Use max_workers=10 in Lambda for 5-8x speedup\n",
      "   âœ… Lambda Memory: 2048 MB (for parallel DuckDB connections)\n",
      "   âœ… Lambda Timeout: 15 minutes\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ðŸ§ª Test 4b: Large Scale Parallel Test (50 symbols) + Lambda Estimation\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ§ª TEST 4b: Large Scale Parallel Consolidation (50 symbols)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test with 50 symbols\n",
    "large_test_count = 50\n",
    "large_symbols = vacuumed_symbols[:large_test_count] if len(vacuumed_symbols) >= large_test_count else vacuumed_symbols\n",
    "print(f\"\\nðŸ“‹ Testing with {len(large_symbols)} symbols using 10 workers\")\n",
    "\n",
    "# Run parallel consolidation\n",
    "large_results, large_time = consolidate_parallel(\n",
    "    symbols=large_symbols,\n",
    "    max_workers=10,\n",
    "    dry_run=True\n",
    ")\n",
    "\n",
    "large_successful = [r for r in large_results if r['status'] == 'success']\n",
    "large_failed = [r for r in large_results if r['status'] == 'failed']\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“Š LARGE SCALE TEST RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Performance Metrics:\")\n",
    "print(f\"   Total symbols: {len(large_results)}\")\n",
    "print(f\"   Successful: {len(large_successful)}\")\n",
    "print(f\"   Failed: {len(large_failed)}\")\n",
    "print(f\"   Total time: {large_time:.2f}s\")\n",
    "print(f\"   Throughput: {len(large_results)/large_time:.1f} symbols/sec\")\n",
    "\n",
    "if large_successful:\n",
    "    total_rows = sum(r['rows_consolidated'] for r in large_successful)\n",
    "    print(f\"   Total rows processed: {total_rows:,}\")\n",
    "    print(f\"   Rows per second: {total_rows/large_time:,.0f}\")\n",
    "\n",
    "# Lambda estimation with parallel processing\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“Š UPDATED LAMBDA ESTIMATION (with Parallel)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if large_successful:\n",
    "    parallel_throughput = len(large_results) / large_time  # symbols per second\n",
    "    \n",
    "    # Lambda limits\n",
    "    LAMBDA_TIMEOUT = 15 * 60  # 15 minutes\n",
    "    LAMBDA_OVERHEAD = 30  # cold start + cleanup\n",
    "    available_time = LAMBDA_TIMEOUT - LAMBDA_OVERHEAD\n",
    "    \n",
    "    # How many symbols can Lambda process with parallel?\n",
    "    symbols_per_lambda = int(parallel_throughput * available_time)\n",
    "    \n",
    "    # Total symbols\n",
    "    TOTAL_SYMBOLS = 5350\n",
    "    lambda_runs_needed = (TOTAL_SYMBOLS + symbols_per_lambda - 1) // symbols_per_lambda\n",
    "    \n",
    "    print(f\"\\nâš¡ With Parallel Processing (10 workers):\")\n",
    "    print(f\"   Throughput: {parallel_throughput:.1f} symbols/sec\")\n",
    "    print(f\"   Symbols per Lambda run: ~{symbols_per_lambda:,}\")\n",
    "    print(f\"   Lambda runs needed for full consolidation: {lambda_runs_needed}\")\n",
    "    \n",
    "    full_time_estimate = TOTAL_SYMBOLS / parallel_throughput\n",
    "    print(f\"\\nðŸš€ Full Consolidation Estimate:\")\n",
    "    print(f\"   Time for all {TOTAL_SYMBOLS:,} symbols: ~{full_time_estimate/60:.1f} min\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Daily Incremental (100 symbols with new data):\")\n",
    "    daily_time = 100 / parallel_throughput\n",
    "    print(f\"   Estimated time: ~{daily_time:.0f}s ({daily_time/60:.1f} min)\")\n",
    "    print(f\"   âœ… Fits easily within 15 min Lambda timeout!\")\n",
    "    \n",
    "    print(f\"\\nðŸ“¦ RECOMMENDATION:\")\n",
    "    print(f\"   âœ… Use max_workers=10 in Lambda for 5-8x speedup\")\n",
    "    print(f\"   âœ… Lambda Memory: 2048 MB (for parallel DuckDB connections)\")\n",
    "    print(f\"   âœ… Lambda Timeout: 15 minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9f2e56",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“‹ Benchmark Summary & Recommendations\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "| Benchmark | Winner | Speedup | Recommendation |\n",
    "|-----------|--------|---------|----------------|\n",
    "| **Resampling** | DuckDB (S3) | ~5-10x vs PostgreSQL | Use DuckDB for OLAP analytics |\n",
    "| **S3 Reading** | Explicit paths | ~50-100x vs wildcards | Avoid `symbol=*` wildcards |\n",
    "| **Consolidation** | Parallel (10 workers) | ~6-8x vs sequential | Use ThreadPoolExecutor |\n",
    "\n",
    "## Production Recommendations\n",
    "\n",
    "1. **Resampler**: DuckDB + S3 data.parquet reads âœ…\n",
    "2. **Consolidator**: Parallel processing with 10 workers âœ…\n",
    "3. **Fetcher**: Continue writing date-partitioned files âœ…\n",
    "4. **S3 Queries**: Always use explicit symbol paths, never wildcards âœ…\n",
    "\n",
    "## Architecture Validated\n",
    "\n",
    "```\n",
    "Lambda Fetcher â†’ S3 Bronze (date=*.parquet)\n",
    "                      â†“\n",
    "Batch Consolidator â†’ S3 Bronze (data.parquet)\n",
    "                      â†“\n",
    "Batch Resampler â†’ S3 Silver (silver_3d, 5d, 8d...)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
